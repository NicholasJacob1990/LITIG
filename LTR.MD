## ğŸš€â€¯ImplementaÃ§Ã£o LTRâ€¯100â€¯%â€¯Automatizada â€” visÃ£o pontaâ€‘aâ€‘ponta

**Parteâ€¯1â€¯/â€¯2â€¯â€“â€¯Dados, Pipeline de Treino e PublicaÃ§Ã£o**

> **Objetivo:** colocar um LightGBMâ€¯(LambdaMART) em produÃ§Ã£o, treinado diariamente, servindo em *shadow* paraâ€¯10â€¯% do trÃ¡fego, com **fallback** automÃ¡tico para a soma de pesos.

---

### 1. InstrumentaÃ§Ã£o de Eventos (diaâ€¯0)

| Evento            | Quando disparar     | Campos mÃ­nimos                                  |
| ----------------- | ------------------- | ----------------------------------------------- |
| `impression`      | API devolve ranking | `case_id`, lista `[lawyer_ids]`, `weights_used` |
| `click_profile`   | UsuÃ¡rio abre perfil | `case_id`, `lawyer_id`, `rank_pos`              |
| `contact_made`    | Cria chat/ligaÃ§Ã£o   | idem acima                                      |
| `contract_signed` | Contrato digital    | idem acima                                      |

*Logar em **Kafka topic** `match_events`; fallback direto em Postgres se ainda nÃ£o tiver Kafka.*

---

### 2. Dataâ€¯Lake & ETL (diaâ€¯1â€¯â†’â€¯âˆ)

1. **Flink** lÃª o tÃ³pico, grava Parquet em
   `s3://datalake/ltr/raw/YYYY/MM/DD/*.parquet`.
2. **Airflowâ€¯DAG** (`etl_ltr_daily`) consolida o diaâ€¯Tâ€‘1:

   * Carrega Parquet, enriquece com KPIs do advogado, diversity, etc.
   * Gera dataset tabular com:

     * coluna `qid`â€¯=â€¯`case_id`
     * coluna `label` (0/0.3/0.6/1)
     * 11â€¯features normalizadas
   * Salva em `s3://datalake/ltr/processed/{date}/matrix.parquet`.

---

### 3. Pipeline de Treinamentoâ€¯(Airflow DAG `train_ltr_daily`)

```mermaid
graph LR
A[Download matrix.parquet] --> B[Split temporal<br>(train/valid/test)]
B --> C[Scaler fit & save<br>(feature_map.json)]
C --> D[Train LightGBM<br>â€”Â LambdaMART]
D --> E[Eval nDCG@5 & MRR]
E --> F{Pass thresholds?}
F -- NO --> G[Fail DAGÂ & alert]
F -- YES --> H[Register inÂ MLflow]
H --> I[Upload model & feature_map<br>to S3 / litgo-models/ltr/{ts}/]
```

**ParÃ¢metros LightGBM bÃ¡sicos**

```python
params = {
    "objective":      "lambdarank",
    "metric":         "ndcg",
    "ndcg_eval_at":   [5],
    "learning_rate":  0.05,
    "num_leaves":     48,
    "min_data_in_leaf": 20,
    "feature_fraction": 0.9,
    "lambda_l1": 0.2,          # regularizaÃ§Ã£o para poucos dados
    "lambda_l2": 0.2,
    "early_stopping_round": 100,
    "seed": 42
}
```

*Com \~500 linhas o treino leva <â€¯30â€¯s em CPU.*

---

### 4. Gate de Qualidade (offline)

| MÃ©trica            | Alvo                | ImplementaÃ§Ã£o                         |
| ------------------ | ------------------- | ------------------------------------- |
| nDCG\@5            | â‰¥â€¯baselineâ€¯+â€¯3â€¯p.p. | `lgb.cv`Â ou funÃ§Ã£o manual             |
| MRR\@5             | â†‘â€¯vs baseline       | idem                                  |
| Fairâ€‘GapÂ Î”         | â‰¤â€¯0.05              | comparar `fair_base` por gÃªnero/etnia |
| LatÃªnciaÂ p95Â (inf) | <â€¯15â€¯ms CPU         | teste local `timeit`                  |

Se **qualquer** mÃ©trica falharÂ â†’ DAG falha â†’ modelo **nÃ£o** Ã© publicado.

---

### 5. PublicaÃ§Ã£o & Versionamento

* Se aprovado, o step `publish_model` faz:

  1. copia `ltr_model.txt` + `feature_map.json` para
     `s3://litgo-models/ltr/{ts}/`
  2. grava registro em **MLflow** (`stage=staging`, tag `ts`).
  3. dispara webhook para o cluster K8s (HelmÂ chart) indicando â€œnovo modelo prontoâ€.

---

### 6. Deploy no ServiÃ§o `/ltr/score`

| Pasta no container         | Valor                                                         |
| -------------------------- | ------------------------------------------------------------- |
| `/opt/models/ltr_current/` | cÃ³pia do Ãºltimo **aprovado** (`model.txt`, `feature_map.pkl`) |

O container reinicia via Rollingâ€¯Update (ArgoÂ Rollouts) e carrega o arquivoâ€¯TXT:

```python
MODEL = lgb.Booster(model_file="/opt/models/ltr_current/model.txt")
FEATURES = pickle.load(open("/opt/models/ltr_current/feature_map.pkl", "rb"))
```

LatÃªncia tÃ­pica: **\~3â€¯ms** p95 em CPUÂ AVX2.

---

### 7. IntegraÃ§Ã£o no `MatchmakingAlgorithm`

```python
async def rank_with_fallback(case, lawyers):
    try:
        feat_matrix = build_feature_matrix(case, lawyers, FEATURES)
        ltr_scores = await call_ltr_service(feat_matrix, timeout=0.2)
        for lw, sc in zip(lawyers, ltr_scores):
            lw.scores["ltr"] = sc
            lw.scores["source"] = "ltr"
        return rerank_fairness(lawyers)
    except Exception as e:
        logger.warning("LTR fail (%s) â€“ fallback to weights", e)
        matcher = MatchmakingAlgorithm()
        return await matcher.rank(case, lawyers, preset="balanced")
```

*Timeout curto garante **failâ€‘fast**; fallback preserva UX.*

## ğŸš€â€¯ImplementaÃ§Ã£o LTRâ€¯100â€¯%â€¯Automatizada â€” visÃ£o pontaâ€‘aâ€‘ponta

**Parteâ€¯2â€¯/â€¯2â€¯â€“â€¯ShadowÂ Traffic, Observabilidade, Rollback & GovernanÃ§a**

---

### 8. Roteamento de TrÃ¡fego (10â€¯% Shadow)

| Camada                            | Como fazer                                                                                                                                                                     | ObservaÃ§Ã£o                                                 |
| --------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------- |
| **API Gateway** (FastAPI)         | Middleware inspeciona `Xâ€‘Shadowâ€‘LTR=1` no header; se presente, faz **duas** chamadas: <br>â‘  ranking legacy (retorna ao usuÃ¡rio) <br>â‘¡ ranking LTR (ignora resposta, mas loga). | Header injetado a partir de um *flag* redis distribuÃ­do.   |
| **Loadâ€‘Balancer** (NGINXâ€‘Ingress) | Regra `map $request_id $shadow`Â (10â€¯% hash) â†’ duplica requisiÃ§Ã£o para `ltr-shadow-svc`.                                                                                        | Serve para linguagens ou times sem controle de middleware. |

*ImplementaÃ§Ã£o escolhida depende do stack de rede; o padrÃ£o mais simples Ã© **middleware no Gateway**.*

---

### 9. PersistÃªncia de Resultados Shadow

```sql
CREATE TABLE ltr_shadow_log (
    shadow_id      UUID PRIMARY KEY,
    ts_utc         TIMESTAMPTZ,
    case_id        UUID,
    user_id        UUID,
    lawyer_id      TEXT,
    rank_legacy    SMALLINT,
    rank_ltr       SMALLINT,
    label          REAL      -- 0/0.3/0.6/1 (preenchido dias depois via ETL)
);
```

*Os ETLs diÃ¡rios atualizam `label` quando eventos de click/contrato forem gerados.*

---

### 10. MÃ©tricas de ComparaÃ§Ã£o

| MÃ©trica online           | CÃ¡lculo                                                             | Painel Grafana            |
| ------------------------ | ------------------------------------------------------------------- | ------------------------- |
| **Hit\@3 Shadow**        | %Â de casos em que advogado contratado teria ficado no Topâ€¯3 do LTR. | Linha diÃ¡ria vs baseline. |
| **MRR Gap**              | `MRR_ltr â€“ MRR_legacy` (em shadow).                                 | Verde se >â€¯0.             |
| **LatÃªncia p95 LTR svc** | Prometheus histogram.                                               | Alerta >â€¯20â€¯ms.           |
| **Fallback rate**        | `ltr_fallback_total / requests`                                     | Alerta >â€¯2â€¯%.             |

*Scripts PySpark geram **relatÃ³rios semanais** (`shadow_eval_{iso_week}.csv`) colocados no Dataâ€¯Lake.*

---

### 11. CritÃ©rios de PromoÃ§Ã£o

1. **DuraÃ§Ã£o mÃ­nima:** â‰¥â€¯14â€¯dias **ou** â‰¥â€¯3â€¯000 casos.
2. **Hit\@3 Shadow:** melhoria â‰¥â€¯5â€¯p.p. sobre legacy.
3. **MRR Gap:** >â€¯0 com 95â€¯%â€¯CI.
4. **Fairâ€‘Gapâ€¯Î”:** nÃ£o piora (>â€¯0.02) vs legacy.
5. **LatÃªncia p95:** <â€¯15â€¯ms CPU.

> Se **todos** ok â†’ *Argo Rollouts*â€¯promove trÃ¡fego LTR para 100â€¯% atravÃ©s de `set-weight 100`.
> Caso contrÃ¡rio, permanece em shadow e o ciclo de reâ€‘treino continua.

---

### 12. Rollback AutomÃ¡tico

```yaml
spec:
  analysis:
    rollback:
      steps:
        - analysisTemplate: latency-check
          threshold: 25ms
        - analysisTemplate: hitrate-check
          threshold: -1pp   # caiu 1 ponto ou mais
```

*Qualquer violaÃ§Ã£o de SLA aciona **automatic rollback** â†’ peso volta aâ€¯0â€¯%.*

---

### 13. Monitoramento & Alertas

| Ferramenta     | Painel / Alerta                                                         |
| -------------- | ----------------------------------------------------------------------- |
| **Grafana**    | â€œLTRâ€¯Shadow Performanceâ€ â€“ 4 mÃ©tricas chave.                            |
| **Prometheus** | Alertmanager envia Slack: `LTR_FALLBACK_RATE_HIGH`, `LTR_LATENCY_HIGH`. |
| **Sentry**     | Captura exceÃ§Ãµes no endpoint `/ltr/score`.                              |

---

### 14. SeguranÃ§a & Conformidade

* **LGPD optâ€‘out**: se usuÃ¡rio solicitar exclusÃ£o â†’ `DELETE FROM match_events WHERE user_id = â€¦`.
* **PseudonimizaÃ§Ã£o**: hash(user\_id, salt) antes de gravar em tabelas shadow.
* **ModelÂ Card**: PDF gerado via script, armazenado na Wiki interna, descreve dados, mÃ©tricas, limitaÃ§Ãµes.

---

### 15. Linha do Tempo (â€œCriticalÂ Pathâ€)

| Semana | Tarefas                                                    |
| ------ | ---------------------------------------------------------- |
| 1      | Instrumentar eventos + topic Kafka                         |
| 2      | ETL Parquet, tabela `match_events`                         |
| 3      | Implementar pipeline `train_ltr_daily`, thresholds offline |
| 4      | Deploy serviÃ§o `/ltr/score` com fallback                   |
| 5      | Ativar 10â€¯% shadow via header ou Ingress                   |
| 6â€‘7    | Coleta de mÃ©tricas, dashboards, alertas                    |
| 8      | Go/Noâ€‘Go â€“ promoÃ§Ã£o a 100â€¯% se critÃ©rios cumpridos         |

---

### 16. Checklist deâ€¯Goâ€‘Live

* [x] Fallback testado (desligar serviÃ§o LTR â†’ ranking legacy ok)
* [x] Dashboards preenchendo em tempo real
* [x] Alertmanager dispara Slack em fallbackâ€¯>â€¯2â€¯%
* [x] Script de rollback manual documentado
* [x] ModelÂ Card publicado
* [x] DPIA LGPD atualizado

---

### ğŸ“¦ Exemplos de implementaÃ§Ã£o (Parteâ€¯3â€¯/â€¯3)

> A seguir, blocos completos prontos para copiarâ€‘colar: DAG do Airflow, serviÃ§o FastAPI, HelmÂ Chart simplificado, regra Prometheus e consulta SQL Hit\@3.

---

## 17. DAG completoÂ (`dags/train_ltr_daily.py`)

```python
"""
Airflow DAG: Treinamento diÃ¡rio do LightGBMÂ LambdaMART
Agendado para 02:15â€¯UTC (23:15 BRT).
"""
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.task_group import TaskGroup
from datetime import datetime, timedelta
from src import etl, preprocess, train, evaluate, registry

default_args = {
    "owner": "mlops",
    "depends_on_past": False,
    "email": ["ml-alerts@litgo.com"],
    "email_on_failure": True,
    "retries": 1,
    "retry_delay": timedelta(minutes=15),
}

with DAG(
    dag_id="train_ltr_daily",
    description="LightGBM LambdaMART â€“ reâ€‘treino automÃ¡tico",
    start_date=datetime(2025, 7, 1),
    schedule_interval="15 2 * * *",
    catchup=False,
    tags=["ltr", "retrain"],
    default_args=default_args,
) as dag:

    with TaskGroup("etl") as tg_etl:
        extract = PythonOperator(
            task_id="extract",
            python_callable=etl.extract_raw_parquet,
        )
        enrich = PythonOperator(
            task_id="enrich",
            python_callable=etl.enrich_kpi_features,
        )
        extract >> enrich

    preprocess_task = PythonOperator(
        task_id="preprocess",
        python_callable=preprocess.build_matrix,
    )

    train_task = PythonOperator(
        task_id="train",
        python_callable=train.train_lgbm,
    )

    eval_task = PythonOperator(
        task_id="evaluate",
        python_callable=evaluate.eval_lgbm,
    )

    publish_task = PythonOperator(
        task_id="publish",
        python_callable=registry.publish_model,
        trigger_rule="all_success",
    )

    tg_etl >> preprocess_task >> train_task >> eval_task >> publish_task
```

---

## 18. ServiÃ§o `/ltr/score` â€“ FastAPI minimal

```python
# app_ltr/main.py
from fastapi import FastAPI, HTTPException
import lightgbm as lgb
import numpy as np, pickle, json, os

MODEL_PATH = os.getenv("MODEL_PATH", "/opt/models/ltr_current/model.txt")
MAP_PATH   = os.getenv("FEATURE_MAP", "/opt/models/ltr_current/feature_map.pkl")

app = FastAPI(title="LitGo LTR Scoring Service")

MODEL = lgb.Booster(model_file=MODEL_PATH)
FEATURE_ORDER = pickle.load(open(MAP_PATH, "rb"))

@app.post("/ltr/score")
async def ltr_score(payload: dict):
    try:
        x = np.array([[payload["features"][f] for f in FEATURE_ORDER]])
        score = MODEL.predict(x, num_iteration=MODEL.best_iteration)[0]
        return {"score": float(score)}
    except KeyError as e:
        raise HTTPException(status_code=422, detail=f"missing feature {e}")
```

*Conteinerize em `Dockerfile` simples (python:3.11â€‘slim) +Â `uvicorn app_ltr.main:app --host 0.0.0.0 --port 8080`.*

---

## 19. Helm Chartâ€¯(resumo)

```yaml
# charts/ltr/values.yaml
image:
  repository: registry.gitlab.com/litgo/ltr-service
  tag: "v0.3.2"
replicaCount: 3

resources:
  limits:
    cpu: "500m"
    memory: "512Mi"

env:
  MODEL_PATH: /models/model.txt
  FEATURE_MAP: /models/feature_map.pkl

shadow:
  weight: 10    # 10â€¯% trÃ¡fego duplicado

# charts/ltr/templates/deployment.yaml (excerto)
...
        volumeMounts:
          - name: model-volume
            mountPath: /models
      volumes:
        - name: model-volume
          persistentVolumeClaim:
            claimName: ltr-model-pvc
```

*ArgoÂ Rollouts manipula `shadow.weight` alterando route no Ingress.*

---

## 20. Regra Prometheus â€“ LatÃªncia & Fallback

```yaml
# prometheus/alert-rules-ltr.yml
groups:
- name: ltr-service
  rules:
  - alert: LTRServiceHighLatency
    expr: histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{job="ltr-service"}[5m])) by (le))
          > 0.020   # 20â€¯ms
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "LatÃªncia p95 do LTR >Â 20â€¯ms"
      runbook: https://wiki.litgo.com/latency-incident

  - alert: LTRFallbackRateHigh
    expr: rate(ltr_fallback_total[10m]) / rate(api_requests_total[10m]) > 0.02
    for: 15m
    labels:
      severity: critical
    annotations:
      summary: "Fallback para algoritmo legado >â€¯2â€¯%"
      runbook: https://wiki.litgo.com/fallback-incident
```

---

## 21. Consulta SQL â€“ Hit\@3 Shadow vs Legacy

```sql
WITH success AS (
  SELECT case_id,
         lawyer_id,
         MAX(label) AS outcome      -- 1.0 se contratado
  FROM   ltr_shadow_log
  WHERE  label = 1.0
  GROUP BY case_id, lawyer_id
),
positions AS (
  SELECT s.case_id,
         s.lawyer_id,
         MIN(s.rank_legacy) FILTER (WHERE s.rank_legacy IS NOT NULL) AS pos_legacy,
         MIN(s.rank_ltr)    FILTER (WHERE s.rank_ltr    IS NOT NULL) AS pos_ltr
  FROM   ltr_shadow_log s
  JOIN   success        u USING (case_id, lawyer_id)
  GROUP BY s.case_id, s.lawyer_id
)
SELECT
  COUNT(*) FILTER (WHERE pos_legacy <= 3) * 1.0 / COUNT(*) AS hit3_legacy,
  COUNT(*) FILTER (WHERE pos_ltr    <= 3) * 1.0 / COUNT(*) AS hit3_ltr
FROM positions;
```

---
### Sim, vocÃª consegue montar **todo o fluxo automÃ¡tico** dentro doâ€¯Cursorâ€¯IDE â€” mas Ã© importante lembrar que aâ€¯IDE em si apenas orquestra o cÃ³digo e os contÃªineres; quem executa a automaÃ§Ã£o contÃ­nua Ã© a stack Docker/Airflow/K8s que vocÃª vai subir a partir do projeto. Abaixo segue um roteiro pragmÃ¡tico:

---

## 1â€¯â–ªï¸ Preparar o ambiente de desenvolvimento no Cursor

1. **Clone o repositÃ³rio**â€¯`ltr_pipeline/` na workspace do Cursor.
2. Crie um **`.devcontainer/`** ou **`docker-compose.yml`** na raiz com:

   * **airflow-scheduler**, **airflow-webserver**, **postgres**
   * **kafka + zookeeper** (ou Redpanda para dev)
   * **minio** (mock S3)
   * **ltr-service** (FastAPI)
3. No Cursor, abra o painel â€œContainersâ€ â†’ `Compose Up`.

   * Isso jÃ¡ levanta Airflow na portaâ€¯8080 e o serviÃ§o LTR naâ€¯8081.

> **Dica:** o Cursor monta seus volumes locais dentro dos contÃªineres, entÃ£o qualquer alteraÃ§Ã£o de cÃ³digo Ã© â€œhotâ€‘reloadâ€.

---

## 2â€¯â–ªï¸ Criar os alvos de automaÃ§Ã£o

### `Makefile` minimal

```make
setup:  ## Instala dependÃªncias Python
	pip install -r requirements.txt

etl:    ## Roda ETL local usando os Parquets mockados
	python -m src.etl

train:  ## Treina LightGBM com warmâ€‘start
	python -m src.train

evaluate:
	python -m src.evaluate

publish:
	python -m src.registry
```

No Cursor, pressione **âŒ˜â‡§P â†’ â€œRun Make Targetâ€¦â€** e escolha `train` para ver se tudo roda sem sair da IDE.

---

## 3â€¯â–ªï¸ Subir o Airflow local

1. No painel de Terminais do Cursor, rode:

   ```bash
   docker compose up -d airflow-init
   docker compose up airflow-scheduler airflow-webserver
   ```

2. Abra `http://localhost:8080` â†’ ative a DAG `train_ltr_daily`.

> Quando salvar o arquivo `dags/train_ltr_daily.py`, o Cursor detecta e recarrega o contÃªiner Airflow automaticamente.

---

## 4â€¯â–ªï¸ Testar o serviÃ§o `/ltr/score`

* Dentro do Cursor: `curl -X POST localhost:8081/ltr/score -d @sample.json`
* Ajuste os **paths** de modelo apontando para `./models/dev/`.

Quando vocÃª altera o modelo e roda `publish`, o script copia o novo artefato para a pasta bindâ€‘mountada `/models`; o FastAPI faz **watch** via `watchgod` e recarrega.

---

## 5â€¯â–ªï¸ Shadowâ€¯Traffic localâ€¯(simulado)

```python
# tests/test_shadow.py
from src.shadow import simulate_shadow
simulate_shadow(n_cases=50)
```

Rode pelo painel de testes do Cursor â†’ veja o log JSON em tempo real no console embutido.

---

## 6â€¯â–ªï¸ Commit, CI e Deploy

* Configure **GitHubÂ Actions** ou **GitLabÂ CI** para:

  1. `docker build` da imagem `ltr-service`.
  2. Push para o registry.
  3. Deploy via **ArgoÂ Rollouts** manifest armazenado em `deploy/`.

O Cursor jÃ¡ mostra os pipelines Git em uma aba lateral, permitindo acompanhar o deploy sem sair da IDE.

---

## 7â€¯â–ªï¸ Agendar em produÃ§Ã£o

* Crie um arquivo Helm `values-prod.yaml` com `shadow.weight: 10`.
* No Cursor, abra o terminal remoto do cluster (`kubectl`) e rode:

  ```bash
  helm upgrade --install ltr charts/ltr -f deploy/values-prod.yaml
  ```

O Cursor exibe os logs dos pods em tempo real, entÃ£o vocÃª consegue verificar fallback ou latÃªncia sem sair do editor.

---

### Checklist rÃ¡pido

* [x] ContÃªineres sobem via **Compose** no Cursor
* [x] DAG Ã© detectada e executa no Airflow local
* [x] Scripts `etl â†’ train â†’ evaluate â†’ publish` rodam via Make
* [x] ServiÃ§o LTR responde em `/ltr/score`
* [x] Fallback testado manualmente (parar contÃªiner LTR â†’ ranking legacy)

Se todos os itens acima passarem no ambiente local da IDE, basta â€œpromoverâ€ a mesma stack para o cluster de staging/produÃ§Ã£o.

---

**ConclusÃ£o:** o Cursor nÃ£o substitui Airflow nem Kubernetes, mas facilita **codar, versionar e observar** todo o pipeline sem trocar de ferramenta. Uma vez que Dockerâ€¯+â€¯Airflow estejam configurados, o treinamento e o shadow jÃ¡ acontecem de forma **100â€¯% automÃ¡tica** â€” seu papel passa a ser apenas revisar dashboards e *mergear* pullâ€‘requests.




