# üìä SPRINT 3: OTIMIZA√á√ïES E FEATURES - PLANO DETALHADO

> **Dura√ß√£o:** 2 semanas (10 dias √∫teis)  
> **Objetivo:** Melhorar performance e adicionar funcionalidades avan√ßadas  
> **Prioridade:** P2 - Otimiza√ß√µes e melhorias  

## üìã VIS√ÉO GERAL

Este sprint foca em **otimiza√ß√µes de performance** e **funcionalidades avan√ßadas** para tornar o sistema mais eficiente e inteligente. Inclui otimiza√ß√µes de queries, an√°lise de sentimento e m√©tricas avan√ßadas.

## üéØ OBJETIVOS DO SPRINT

### Principais Entregas
1. **Performance Otimizada**: Lat√™ncia <2s para opera√ß√µes cr√≠ticas
2. **An√°lise de Sentimento**: Feedback dos clientes analisado automaticamente
3. **M√©tricas Avan√ßadas**: Relat√≥rios automatizados e insights de neg√≥cio
4. **Sistema Completamente Aut√¥nomo**: Funciona sem interven√ß√£o humana

### M√©tricas de Sucesso
- [ ] Lat√™ncia m√©dia <2s para matching
- [ ] An√°lise de sentimento funcionando
- [ ] Relat√≥rios automatizados gerados
- [ ] Performance 50% melhor que baseline

## üìä √âPICOS E USER STORIES

### ‚ö° EPIC 3.1: Otimiza√ß√µes de Performance
**Problema:** Sistema pode ser mais r√°pido e eficiente

#### US-3.1.1: Otimizar queries de matching
**Como** sistema  
**Quero** executar queries de matching mais rapidamente  
**Para que** a lat√™ncia seja menor que 2 segundos  

**Crit√©rios de Aceita√ß√£o:**
- [ ] Queries de matching otimizadas
- [ ] √çndices espec√≠ficos criados
- [ ] Cache de resultados implementado
- [ ] Lat√™ncia <2s para 95% das requisi√ß√µes

**Implementa√ß√£o:**
```sql
-- √çndices otimizados para matching
CREATE INDEX CONCURRENTLY idx_lawyers_matching_composite 
ON lawyers (status, tags_expertise, cases_30d, geo_latlon) 
WHERE status = 'active';

-- √çndice para queries geogr√°ficas
CREATE INDEX CONCURRENTLY idx_lawyers_geo_performance 
ON lawyers USING GIST (geo_latlon) 
WHERE status = 'active' AND geo_latlon IS NOT NULL;

-- √çndice para features de qualidade
CREATE INDEX CONCURRENTLY idx_lawyers_quality_metrics 
ON lawyers (avaliacao_media, total_cases, success_rate) 
WHERE status = 'active';
```

```python
# backend/services/match_service.py
class MatchServiceOptimized:
    def __init__(self):
        self.cache = TTLCache(maxsize=1000, ttl=300)  # 5 min cache
    
    async def find_matches_optimized(self, case_data: dict) -> List[dict]:
        """Vers√£o otimizada do matching com cache"""
        cache_key = self._generate_cache_key(case_data)
        
        # Verificar cache primeiro
        if cache_key in self.cache:
            cache_hits.inc()
            return self.cache[cache_key]
        
        # Query otimizada com LIMIT precoce
        query = """
        SELECT l.*, 
               l.geo_latlon <-> ST_Point(%s, %s) as distance,
               similarity_score(l.tags_expertise, %s) as area_score
        FROM lawyers l
        WHERE l.status = 'active'
          AND l.tags_expertise && %s  -- Filtro precoce por √°rea
          AND l.cases_30d < l.capacidade_mensal  -- Filtro de capacidade
          AND ST_DWithin(l.geo_latlon, ST_Point(%s, %s), %s)  -- Filtro geogr√°fico
        ORDER BY 
          area_score DESC,
          distance ASC,
          l.avaliacao_media DESC
        LIMIT 50  -- Limitar resultados precocemente
        """
        
        # Executar query otimizada
        matches = await self.execute_optimized_query(query, case_data)
        
        # Cache resultado
        self.cache[cache_key] = matches
        cache_misses.inc()
        
        return matches
```

**Estimativa:** 2 dias

#### US-3.1.2: Paraleliza√ß√£o de embeddings
**Como** sistema  
**Quero** processar m√∫ltiplos embeddings simultaneamente  
**Para que** o throughput seja maior  

**Crit√©rios de Aceita√ß√£o:**
- [ ] Pool de conex√µes OpenAI implementado
- [ ] Processamento paralelo de embeddings
- [ ] Batch processing para m√∫ltiplos casos
- [ ] Throughput 3x maior que baseline

**Implementa√ß√£o:**
```python
# backend/services/embedding_service_parallel.py
import asyncio
from asyncio import Semaphore

class ParallelEmbeddingService:
    def __init__(self):
        self.semaphore = Semaphore(10)  # M√°ximo 10 requests simult√¢neos
        self.session_pool = [
            httpx.AsyncClient(timeout=30) for _ in range(5)
        ]
    
    async def generate_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
        """Gera embeddings em paralelo para m√∫ltiplos textos"""
        async with self.semaphore:
            tasks = []
            for i, text in enumerate(texts):
                session = self.session_pool[i % len(self.session_pool)]
                task = self._generate_single_embedding(session, text)
                tasks.append(task)
            
            embeddings = await asyncio.gather(*tasks, return_exceptions=True)
            
            # Filtrar erros e retornar apenas sucessos
            valid_embeddings = [
                emb for emb in embeddings 
                if not isinstance(emb, Exception)
            ]
            
            return valid_embeddings
    
    async def _generate_single_embedding(self, session: httpx.AsyncClient, text: str) -> List[float]:
        """Gera embedding para um texto espec√≠fico"""
        try:
            response = await session.post(
                "https://api.openai.com/v1/embeddings",
                json={
                    "model": "text-embedding-3-small",
                    "input": text
                },
                headers={"Authorization": f"Bearer {OPENAI_API_KEY}"}
            )
            
            data = response.json()
            return data["data"][0]["embedding"]
            
        except Exception as e:
            logger.error(f"Embedding failed for text: {e}")
            raise
```

**Estimativa:** 1.5 dias

#### US-3.1.3: Compress√£o de dados
**Como** sistema  
**Quero** otimizar o armazenamento de vetores  
**Para que** o banco seja mais eficiente  

**Crit√©rios de Aceita√ß√£o:**
- [ ] Compress√£o de embeddings implementada
- [ ] Redu√ß√£o de 40% no espa√ßo de armazenamento
- [ ] Performance de queries mantida
- [ ] Processo de migra√ß√£o documentado

**Implementa√ß√£o:**
```python
# backend/services/vector_compression.py
import numpy as np
from sklearn.decomposition import PCA

class VectorCompressionService:
    def __init__(self):
        self.pca = PCA(n_components=512)  # Reduzir de 1536 para 512
        self.is_fitted = False
    
    def fit_compression(self, embeddings: List[List[float]]):
        """Treina o modelo de compress√£o"""
        embeddings_array = np.array(embeddings)
        self.pca.fit(embeddings_array)
        self.is_fitted = True
        
        # Salvar modelo treinado
        joblib.dump(self.pca, 'models/pca_compression.pkl')
    
    def compress_embedding(self, embedding: List[float]) -> List[float]:
        """Comprime um embedding usando PCA"""
        if not self.is_fitted:
            self.pca = joblib.load('models/pca_compression.pkl')
            self.is_fitted = True
        
        embedding_array = np.array(embedding).reshape(1, -1)
        compressed = self.pca.transform(embedding_array)
        return compressed[0].tolist()
    
    def decompress_embedding(self, compressed_embedding: List[float]) -> List[float]:
        """Descomprime um embedding (aproxima√ß√£o)"""
        compressed_array = np.array(compressed_embedding).reshape(1, -1)
        decompressed = self.pca.inverse_transform(compressed_array)
        return decompressed[0].tolist()
```

```sql
-- Migra√ß√£o para embeddings comprimidos
ALTER TABLE cases ADD COLUMN embedding_compressed FLOAT8[];
ALTER TABLE lawyers ADD COLUMN embedding_compressed FLOAT8[];

-- Criar √≠ndices para embeddings comprimidos
CREATE INDEX idx_cases_embedding_compressed 
ON cases USING ivfflat (embedding_compressed vector_cosine_ops);
```

**Estimativa:** 2 dias

---

### üß† EPIC 3.2: An√°lise de Sentimento
**Problema:** Feedback dos clientes n√£o √© analisado automaticamente

#### US-3.2.1: Implementar an√°lise de reviews
**Como** sistema  
**Quero** analisar automaticamente o sentimento dos coment√°rios  
**Para que** possa identificar problemas e melhorias  

**Crit√©rios de Aceita√ß√£o:**
- [ ] An√°lise de sentimento implementada
- [ ] Classifica√ß√£o em positivo/neutro/negativo
- [ ] Extra√ß√£o de t√≥picos principais
- [ ] Integra√ß√£o com job de reviews

**Implementa√ß√£o:**
```python
# backend/services/sentiment_analysis.py
from transformers import pipeline
import nltk
from collections import Counter

class SentimentAnalysisService:
    def __init__(self):
        # Usar modelo pr√©-treinado para portugu√™s
        self.sentiment_pipeline = pipeline(
            "sentiment-analysis",
            model="cardiffnlp/twitter-roberta-base-sentiment-latest",
            device=0 if torch.cuda.is_available() else -1
        )
        
        # Baixar recursos do NLTK
        nltk.download('stopwords')
        nltk.download('punkt')
        
        self.stop_words = set(nltk.corpus.stopwords.words('portuguese'))
    
    def analyze_sentiment(self, text: str) -> dict:
        """Analisa sentimento de um texto"""
        try:
            result = self.sentiment_pipeline(text)[0]
            
            # Mapear labels para portugu√™s
            label_map = {
                'LABEL_0': 'negativo',
                'LABEL_1': 'neutro', 
                'LABEL_2': 'positivo'
            }
            
            sentiment = label_map.get(result['label'], 'neutro')
            confidence = result['score']
            
            return {
                'sentiment': sentiment,
                'confidence': confidence,
                'raw_result': result
            }
            
        except Exception as e:
            logger.error(f"Sentiment analysis failed: {e}")
            return {
                'sentiment': 'neutro',
                'confidence': 0.0,
                'error': str(e)
            }
    
    def extract_topics(self, text: str) -> List[str]:
        """Extrai t√≥picos principais do texto"""
        # Tokeniza√ß√£o e limpeza
        tokens = nltk.word_tokenize(text.lower())
        tokens = [t for t in tokens if t.isalpha() and t not in self.stop_words]
        
        # Contar frequ√™ncia de palavras
        word_freq = Counter(tokens)
        
        # Retornar top 5 palavras mais frequentes
        return [word for word, freq in word_freq.most_common(5)]
    
    def analyze_review_batch(self, reviews: List[dict]) -> List[dict]:
        """Analisa m√∫ltiplas reviews em batch"""
        results = []
        
        for review in reviews:
            comment = review.get('comment', '')
            if not comment:
                continue
            
            # An√°lise de sentimento
            sentiment_result = self.analyze_sentiment(comment)
            
            # Extra√ß√£o de t√≥picos
            topics = self.extract_topics(comment)
            
            results.append({
                'review_id': review['id'],
                'sentiment': sentiment_result['sentiment'],
                'confidence': sentiment_result['confidence'],
                'topics': topics,
                'processed_at': datetime.now().isoformat()
            })
        
        return results
```

**Estimativa:** 2 dias

#### US-3.2.2: Integrar ao algoritmo
**Como** sistema  
**Quero** usar an√°lise de sentimento como feature adicional  
**Para que** o matching considere satisfa√ß√£o dos clientes  

**Crit√©rios de Aceita√ß√£o:**
- [ ] Feature de sentimento adicionada ao algoritmo
- [ ] Peso configur√°vel para sentimento
- [ ] Integra√ß√£o com pipeline de matching
- [ ] Testes de impacto na qualidade

**Implementa√ß√£o:**
```python
# backend/algoritmo_match.py - Adicionar feature de sentimento
def sentiment_score(lawyer_data: dict) -> float:
    """Calcula score baseado na an√°lise de sentimento das reviews"""
    reviews = lawyer_data.get('recent_reviews', [])
    if not reviews:
        return 0.5  # Neutro para advogados sem reviews
    
    sentiment_scores = []
    for review in reviews:
        sentiment = review.get('sentiment_analysis', {})
        if sentiment.get('sentiment') == 'positivo':
            sentiment_scores.append(sentiment.get('confidence', 0.8))
        elif sentiment.get('sentiment') == 'negativo':
            sentiment_scores.append(-sentiment.get('confidence', 0.8))
        else:  # neutro
            sentiment_scores.append(0.0)
    
    # M√©dia ponderada por rec√™ncia
    if sentiment_scores:
        # Dar mais peso para reviews mais recentes
        weights = [0.5 ** i for i in range(len(sentiment_scores))]
        weighted_score = sum(s * w for s, w in zip(sentiment_scores, weights))
        total_weight = sum(weights)
        return max(0, min(1, (weighted_score / total_weight + 1) / 2))
    
    return 0.5

# Atualizar c√°lculo de fair_score
def calculate_fair_score(lawyer_data: dict, case_data: dict, weights: dict) -> float:
    """Calcula score final incluindo sentimento"""
    # ... features existentes ...
    
    # Nova feature de sentimento
    sentiment_feature = sentiment_score(lawyer_data)
    
    # Atualizar c√°lculo
    raw_score = (
        weights['area'] * area_score +
        weights['similarity'] * similarity_score +
        weights['track_record'] * track_record_score +
        weights['geography'] * geography_score +
        weights['quality'] * quality_score +
        weights['urgency'] * urgency_score +
        weights['reviews'] * reviews_score +
        weights['cost'] * cost_score +
        weights.get('sentiment', 0.05) * sentiment_feature  # Novo peso
    )
    
    # ... resto do c√°lculo ...
```

**Estimativa:** 1 dia

---

### üìà EPIC 3.3: M√©tricas Avan√ßadas
**Problema:** Falta insights avan√ßados sobre o neg√≥cio

#### US-3.3.1: Implementar m√©tricas de neg√≥cio
**Como** gestor  
**Quero** ter m√©tricas avan√ßadas sobre o neg√≥cio  
**Para que** possa tomar decis√µes baseadas em dados  

**Crit√©rios de Aceita√ß√£o:**
- [ ] M√©tricas de convers√£o implementadas
- [ ] An√°lise de funil de vendas
- [ ] Segmenta√ß√£o por √°rea jur√≠dica
- [ ] M√©tricas de satisfa√ß√£o do cliente

**Implementa√ß√£o:**
```python
# backend/services/business_metrics.py
class BusinessMetricsService:
    def __init__(self):
        self.supabase = get_supabase_client()
    
    async def calculate_conversion_metrics(self, period_days: int = 30) -> dict:
        """Calcula m√©tricas de convers√£o"""
        start_date = datetime.now() - timedelta(days=period_days)
        
        # Funil de convers√£o
        total_cases = await self._count_cases_since(start_date)
        cases_with_offers = await self._count_cases_with_offers_since(start_date)
        offers_accepted = await self._count_offers_accepted_since(start_date)
        contracts_signed = await self._count_contracts_signed_since(start_date)
        
        # Calcular taxas
        offer_rate = cases_with_offers / total_cases if total_cases > 0 else 0
        acceptance_rate = offers_accepted / cases_with_offers if cases_with_offers > 0 else 0
        signing_rate = contracts_signed / offers_accepted if offers_accepted > 0 else 0
        
        return {
            'period_days': period_days,
            'total_cases': total_cases,
            'cases_with_offers': cases_with_offers,
            'offers_accepted': offers_accepted,
            'contracts_signed': contracts_signed,
            'offer_rate': offer_rate,
            'acceptance_rate': acceptance_rate,
            'signing_rate': signing_rate,
            'overall_conversion': contracts_signed / total_cases if total_cases > 0 else 0
        }
    
    async def analyze_by_legal_area(self, period_days: int = 30) -> dict:
        """An√°lise segmentada por √°rea jur√≠dica"""
        start_date = datetime.now() - timedelta(days=period_days)
        
        query = """
        SELECT 
            c.area,
            COUNT(DISTINCT c.id) as total_cases,
            COUNT(DISTINCT o.id) as total_offers,
            COUNT(DISTINCT CASE WHEN o.status = 'interested' THEN o.id END) as accepted_offers,
            COUNT(DISTINCT ct.id) as signed_contracts,
            AVG(r.rating) as avg_rating,
            AVG(EXTRACT(EPOCH FROM (o.updated_at - o.created_at))/3600) as avg_response_time_hours
        FROM cases c
        LEFT JOIN offers o ON c.id = o.case_id
        LEFT JOIN contracts ct ON c.id = ct.case_id
        LEFT JOIN reviews r ON ct.id = r.contract_id
        WHERE c.created_at >= %s
        GROUP BY c.area
        ORDER BY total_cases DESC
        """
        
        results = await self.supabase.rpc('execute_sql', {'query': query, 'params': [start_date]})
        
        return {
            'period_days': period_days,
            'areas': results.data
        }
    
    async def calculate_lawyer_performance(self, period_days: int = 30) -> dict:
        """M√©tricas de performance dos advogados"""
        start_date = datetime.now() - timedelta(days=period_days)
        
        query = """
        SELECT 
            l.id,
            l.name,
            COUNT(DISTINCT o.id) as offers_received,
            COUNT(DISTINCT CASE WHEN o.status = 'interested' THEN o.id END) as offers_accepted,
            COUNT(DISTINCT ct.id) as contracts_signed,
            AVG(r.rating) as avg_rating,
            AVG(EXTRACT(EPOCH FROM (o.updated_at - o.created_at))/3600) as avg_response_time_hours,
            SUM(ct.value) as total_revenue
        FROM lawyers l
        LEFT JOIN offers o ON l.id = o.lawyer_id AND o.created_at >= %s
        LEFT JOIN contracts ct ON l.id = ct.lawyer_id AND ct.created_at >= %s
        LEFT JOIN reviews r ON ct.id = r.contract_id
        WHERE l.status = 'active'
        GROUP BY l.id, l.name
        HAVING COUNT(DISTINCT o.id) > 0
        ORDER BY contracts_signed DESC, avg_rating DESC
        """
        
        results = await self.supabase.rpc('execute_sql', {'query': query, 'params': [start_date, start_date]})
        
        return {
            'period_days': period_days,
            'lawyers': results.data
        }
```

**Estimativa:** 2 dias

#### US-3.3.2: Relat√≥rios automatizados
**Como** gestor  
**Quero** receber relat√≥rios automatizados  
**Para que** possa acompanhar o neg√≥cio sem esfor√ßo manual  

**Crit√©rios de Aceita√ß√£o:**
- [ ] Relat√≥rios semanais automatizados
- [ ] Relat√≥rios mensais detalhados
- [ ] Alertas para m√©tricas cr√≠ticas
- [ ] Distribui√ß√£o por email

**Implementa√ß√£o:**
```python
# backend/jobs/automated_reports.py
from jinja2 import Template
import matplotlib.pyplot as plt
import seaborn as sns

class AutomatedReportsService:
    def __init__(self):
        self.business_metrics = BusinessMetricsService()
        self.email_service = EmailService()
    
    async def generate_weekly_report(self):
        """Gera relat√≥rio semanal automatizado"""
        # Coletar m√©tricas
        conversion_metrics = await self.business_metrics.calculate_conversion_metrics(7)
        area_analysis = await self.business_metrics.analyze_by_legal_area(7)
        lawyer_performance = await self.business_metrics.calculate_lawyer_performance(7)
        
        # Gerar gr√°ficos
        charts = await self._generate_charts(conversion_metrics, area_analysis)
        
        # Gerar relat√≥rio HTML
        report_html = await self._generate_report_html({
            'period': 'Semanal',
            'conversion_metrics': conversion_metrics,
            'area_analysis': area_analysis,
            'lawyer_performance': lawyer_performance,
            'charts': charts
        })
        
        # Enviar por email
        await self.email_service.send_report(
            to=['gestao@litgo.com'],
            subject='Relat√≥rio Semanal - LITGO5',
            html_content=report_html
        )
    
    async def generate_monthly_report(self):
        """Gera relat√≥rio mensal detalhado"""
        # M√©tricas mensais
        conversion_metrics = await self.business_metrics.calculate_conversion_metrics(30)
        area_analysis = await self.business_metrics.analyze_by_legal_area(30)
        lawyer_performance = await self.business_metrics.calculate_lawyer_performance(30)
        
        # An√°lises adicionais para relat√≥rio mensal
        trend_analysis = await self._analyze_trends()
        satisfaction_analysis = await self._analyze_satisfaction()
        
        # Gerar relat√≥rio completo
        report_html = await self._generate_detailed_report({
            'period': 'Mensal',
            'conversion_metrics': conversion_metrics,
            'area_analysis': area_analysis,
            'lawyer_performance': lawyer_performance,
            'trend_analysis': trend_analysis,
            'satisfaction_analysis': satisfaction_analysis
        })
        
        # Enviar para stakeholders
        await self.email_service.send_report(
            to=['gestao@litgo.com', 'comercial@litgo.com'],
            subject='Relat√≥rio Mensal - LITGO5',
            html_content=report_html
        )
    
    async def _generate_charts(self, conversion_metrics: dict, area_analysis: dict) -> dict:
        """Gera gr√°ficos para o relat√≥rio"""
        charts = {}
        
        # Gr√°fico de funil de convers√£o
        plt.figure(figsize=(10, 6))
        funnel_data = [
            conversion_metrics['total_cases'],
            conversion_metrics['cases_with_offers'],
            conversion_metrics['offers_accepted'],
            conversion_metrics['contracts_signed']
        ]
        funnel_labels = ['Casos', 'Ofertas', 'Aceites', 'Contratos']
        
        plt.bar(funnel_labels, funnel_data, color=['#3498db', '#2ecc71', '#f39c12', '#e74c3c'])
        plt.title('Funil de Convers√£o - √öltimos 7 dias')
        plt.ylabel('Quantidade')
        
        # Salvar gr√°fico
        chart_path = f'/tmp/conversion_funnel_{datetime.now().strftime("%Y%m%d")}.png'
        plt.savefig(chart_path)
        charts['conversion_funnel'] = chart_path
        
        # Gr√°fico por √°rea jur√≠dica
        plt.figure(figsize=(12, 6))
        areas = [area['area'] for area in area_analysis['areas']]
        cases = [area['total_cases'] for area in area_analysis['areas']]
        
        plt.bar(areas, cases, color='#3498db')
        plt.title('Casos por √Årea Jur√≠dica - √öltimos 7 dias')
        plt.ylabel('N√∫mero de Casos')
        plt.xticks(rotation=45)
        
        chart_path = f'/tmp/cases_by_area_{datetime.now().strftime("%Y%m%d")}.png'
        plt.savefig(chart_path, bbox_inches='tight')
        charts['cases_by_area'] = chart_path
        
        return charts
    
    async def _generate_report_html(self, data: dict) -> str:
        """Gera HTML do relat√≥rio"""
        template = Template("""
        <!DOCTYPE html>
        <html>
        <head>
            <title>Relat√≥rio {{ data.period }} - LITGO5</title>
            <style>
                body { font-family: Arial, sans-serif; margin: 20px; }
                .metric { background: #f8f9fa; padding: 15px; margin: 10px 0; border-radius: 5px; }
                .chart { text-align: center; margin: 20px 0; }
                table { width: 100%; border-collapse: collapse; }
                th, td { padding: 8px; text-align: left; border-bottom: 1px solid #ddd; }
                th { background-color: #f2f2f2; }
            </style>
        </head>
        <body>
            <h1>Relat√≥rio {{ data.period }} - LITGO5</h1>
            
            <h2>M√©tricas de Convers√£o</h2>
            <div class="metric">
                <h3>Funil de Convers√£o</h3>
                <p>Total de Casos: {{ data.conversion_metrics.total_cases }}</p>
                <p>Taxa de Ofertas: {{ "%.1f"|format(data.conversion_metrics.offer_rate * 100) }}%</p>
                <p>Taxa de Aceita√ß√£o: {{ "%.1f"|format(data.conversion_metrics.acceptance_rate * 100) }}%</p>
                <p>Taxa de Assinatura: {{ "%.1f"|format(data.conversion_metrics.signing_rate * 100) }}%</p>
                <p><strong>Convers√£o Geral: {{ "%.1f"|format(data.conversion_metrics.overall_conversion * 100) }}%</strong></p>
            </div>
            
            <h2>Performance por √Årea Jur√≠dica</h2>
            <table>
                <tr>
                    <th>√Årea</th>
                    <th>Casos</th>
                    <th>Ofertas</th>
                    <th>Contratos</th>
                    <th>Avalia√ß√£o M√©dia</th>
                </tr>
                {% for area in data.area_analysis.areas %}
                <tr>
                    <td>{{ area.area }}</td>
                    <td>{{ area.total_cases }}</td>
                    <td>{{ area.total_offers }}</td>
                    <td>{{ area.signed_contracts }}</td>
                    <td>{{ "%.1f"|format(area.avg_rating or 0) }}</td>
                </tr>
                {% endfor %}
            </table>
            
            <p><em>Relat√≥rio gerado automaticamente em {{ datetime.now().strftime("%d/%m/%Y %H:%M") }}</em></p>
        </body>
        </html>
        """)
        
        return template.render(data=data, datetime=datetime)
```

**Estimativa:** 2 dias

## üìÖ CRONOGRAMA DETALHADO

### Semana 1 (Dias 1-5)
| Dia | Atividade | Respons√°vel | Status |
|:---:|:---|:---|:---:|
| 1 | US-3.1.1: Otimizar queries matching | Dev Backend | ‚è≥ |
| 2 | US-3.1.1: Continuar otimiza√ß√µes | Dev Backend | ‚è≥ |
| 2 | US-3.1.2: Paraleliza√ß√£o embeddings | Dev Backend | ‚è≥ |
| 3 | US-3.1.2: Continuar paraleliza√ß√£o | Dev Backend | ‚è≥ |
| 4 | US-3.1.3: Compress√£o de dados | Dev Backend | ‚è≥ |
| 5 | US-3.1.3: Continuar compress√£o | Dev Backend | ‚è≥ |

### Semana 2 (Dias 6-10)
| Dia | Atividade | Respons√°vel | Status |
|:---:|:---|:---|:---:|
| 6 | US-3.2.1: An√°lise de sentimento | Dev Backend | ‚è≥ |
| 7 | US-3.2.1: Continuar an√°lise | Dev Backend | ‚è≥ |
| 7 | US-3.2.2: Integrar ao algoritmo | Dev Backend | ‚è≥ |
| 8 | US-3.3.1: M√©tricas de neg√≥cio | Dev Backend | ‚è≥ |
| 9 | US-3.3.1: Continuar m√©tricas | Dev Backend | ‚è≥ |
| 10 | US-3.3.2: Relat√≥rios automatizados | Dev Backend | ‚è≥ |

## üß™ ESTRAT√âGIA DE TESTES

### Testes de Performance
- [ ] Benchmarks de lat√™ncia antes/depois
- [ ] Testes de carga com queries otimizadas
- [ ] Testes de throughput paralelo
- [ ] Testes de compress√£o/descompress√£o

### Testes de An√°lise de Sentimento
- [ ] Testes com dataset de reviews reais
- [ ] Valida√ß√£o de precis√£o do modelo
- [ ] Testes de integra√ß√£o com algoritmo
- [ ] Testes de performance do NLP

### Testes de Relat√≥rios
- [ ] Testes de gera√ß√£o de m√©tricas
- [ ] Valida√ß√£o de c√°lculos
- [ ] Testes de templates HTML
- [ ] Testes de envio de email

## üöÄ CRIT√âRIOS DE ACEITA√á√ÉO DO SPRINT

### Performance
- [ ] **Lat√™ncia <2s**: 95% das opera√ß√µes de matching em <2s
- [ ] **Throughput 3x**: Processamento paralelo 3x mais r√°pido
- [ ] **Compress√£o 40%**: Redu√ß√£o de 40% no armazenamento
- [ ] **Queries otimizadas**: √çndices e cache implementados

### An√°lise de Sentimento
- [ ] **Precis√£o >80%**: Modelo com precis√£o >80% em dataset teste
- [ ] **Integra√ß√£o completa**: Feature de sentimento no algoritmo
- [ ] **Processamento autom√°tico**: Reviews analisadas automaticamente
- [ ] **T√≥picos extra√≠dos**: Principais temas identificados

### M√©tricas e Relat√≥rios
- [ ] **M√©tricas completas**: Convers√£o, √°rea, performance implementadas
- [ ] **Relat√≥rios autom√°ticos**: Semanal e mensal funcionando
- [ ] **Visualiza√ß√µes**: Gr√°ficos e tabelas gerados
- [ ] **Distribui√ß√£o**: Emails enviados automaticamente

### Sistema Aut√¥nomo
- [ ] **Zero interven√ß√£o**: Sistema funciona sem interven√ß√£o manual
- [ ] **Monitoramento completo**: Todas as m√©tricas coletadas
- [ ] **Alertas ativos**: Problemas detectados automaticamente
- [ ] **Relat√≥rios regulares**: Stakeholders informados automaticamente

## üîß CONFIGURA√á√ÉO DE AMBIENTE

### Vari√°veis de Ambiente Adicionais
```bash
# Performance
QUERY_CACHE_TTL=300
EMBEDDING_BATCH_SIZE=10
COMPRESSION_ENABLED=true
PCA_COMPONENTS=512

# An√°lise de Sentimento
SENTIMENT_MODEL=cardiffnlp/twitter-roberta-base-sentiment-latest
SENTIMENT_DEVICE=cpu
TOPIC_EXTRACTION_ENABLED=true

# Relat√≥rios
REPORTS_ENABLED=true
WEEKLY_REPORT_DAY=monday
MONTHLY_REPORT_DAY=1
REPORT_RECIPIENTS=gestao@litgo.com,comercial@litgo.com
```

### Depend√™ncias Adicionais
```txt
# requirements.txt - Adicionar
transformers==4.35.0
torch==2.0.1
scikit-learn==1.3.0
matplotlib==3.7.1
seaborn==0.12.2
jinja2==3.1.2
nltk==3.8.1
```

## üìä M√âTRICAS DE SUCESSO

### Performance
- **Lat√™ncia P95**: <2s (baseline: 5s)
- **Throughput**: 3x maior (baseline: 10 req/s ‚Üí 30 req/s)
- **Armazenamento**: 40% redu√ß√£o
- **Cache Hit Rate**: >70%

### Qualidade
- **Precis√£o Sentimento**: >80%
- **Cobertura T√≥picos**: >90% das reviews
- **Acur√°cia M√©tricas**: 100% (valida√ß√£o manual)

### Automa√ß√£o
- **Relat√≥rios Entregues**: 100% no prazo
- **Alertas Funcionais**: <5% falsos positivos
- **Uptime**: >99.9%

## üéØ DEFINI√á√ÉO DE PRONTO

Uma user story est√° pronta quando:
- [ ] C√≥digo otimizado implementado
- [ ] Testes de performance passando
- [ ] Benchmarks documentados
- [ ] M√©tricas coletadas
- [ ] Documenta√ß√£o t√©cnica atualizada
- [ ] Code review aprovado
- [ ] Deploy em staging validado
- [ ] Impacto na performance medido

## üìû PR√ìXIMOS PASSOS

### Ap√≥s Sprint 3
1. **Monitoramento de Performance**: Acompanhar m√©tricas em produ√ß√£o
2. **Otimiza√ß√µes Cont√≠nuas**: Baseadas em dados reais
3. **Expans√£o de Features**: Novas funcionalidades baseadas em feedback

### Riscos e Mitiga√ß√µes
- **Risco**: Otimiza√ß√µes introduzem bugs
  - **Mitiga√ß√£o**: Testes extensivos e deploy gradual
- **Risco**: An√°lise de sentimento com baixa precis√£o
  - **Mitiga√ß√£o**: Valida√ß√£o com dataset real e fine-tuning
- **Risco**: Relat√≥rios com dados incorretos
  - **Mitiga√ß√£o**: Valida√ß√£o cruzada e testes manuais

---

**üìä Este sprint consolida o sistema como uma solu√ß√£o completa, eficiente e inteligente para matching jur√≠dico.** 