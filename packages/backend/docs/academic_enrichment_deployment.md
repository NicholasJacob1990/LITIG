# Academic Enrichment - Guia de Deployment

## üéØ **Configura√ß√µes Dispon√≠veis**

O Academic Enrichment oferece **4 n√≠veis de configura√ß√£o** com fallback gracioso:

### **N√≠vel 0: Desabilitado (Padr√£o)**
```bash
# N√£o instalar depend√™ncias
# pip install aiohttp aiolimiter unidecode  # ‚Üê N√ÉO executar

# Ou n√£o configurar APIs
export PERPLEXITY_API_KEY=""
export OPENAI_DEEP_KEY=""
```
**Resultado:**
- ‚úÖ Sistema funciona normalmente
- ‚úÖ Feature Q usa l√≥gica original (experi√™ncia + t√≠tulos + publica√ß√µes)
- ‚úÖ Zero lat√™ncia adicional
- ‚úÖ Zero custos de API

### **N√≠vel 1: Apenas Perplexity (Recomendado MVP)**
```bash
# Instalar depend√™ncias
pip install aiohttp aiolimiter unidecode

# Configurar apenas Perplexity
export PERPLEXITY_API_KEY="your_perplexity_key"
export OPENAI_DEEP_KEY=""  # N√£o necess√°rio
```
**Resultado:**
- ‚úÖ Universidades avaliadas por ranking QS/CAPES
- ‚úÖ Peri√≥dicos avaliados por Qualis/SJR
- ‚úÖ Custo baixo (~$50-100/m√™s)
- ‚úÖ Lat√™ncia aceit√°vel (+300-800ms primeira vez)
- ‚ö†Ô∏è Sem fallback para peri√≥dicos n√£o resolvidos

### **N√≠vel 2: Apenas Deep Research (Nicho)**
```bash
# Instalar depend√™ncias
pip install aiohttp aiolimiter unidecode

# Configurar apenas OpenAI
export PERPLEXITY_API_KEY=""  # N√£o necess√°rio
export OPENAI_DEEP_KEY="your_openai_key"
```
**Resultado:**
- ‚úÖ Precis√£o m√°xima para casos especiais
- ‚ö†Ô∏è Muito caro ($20/task)
- ‚ö†Ô∏è Muito lento (5-15 min/item)
- ‚ùå N√£o recomendado para uso geral

### **N√≠vel 3: H√≠brido (Premium)**
```bash
# Instalar depend√™ncias
pip install aiohttp aiolimiter unidecode

# Configurar ambas APIs
export PERPLEXITY_API_KEY="your_perplexity_key"
export OPENAI_DEEP_KEY="your_openai_key"
```
**Resultado:**
- ‚úÖ Cobertura m√°xima (Perplexity + fallback Deep Research)
- ‚úÖ Otimiza√ß√£o de custos (Deep Research s√≥ quando necess√°rio)
- ‚úÖ Qualidade premium para casos top-tier
- ‚ö†Ô∏è Custo m√©dio-alto (~$200-500/m√™s)

## üìä **Matriz de Decis√£o**

| Cen√°rio | Config Recomendada | Custo/M√™s | Lat√™ncia | Cobertura |
|---------|-------------------|------------|----------|-----------|
| **MVP/Teste** | N√≠vel 0 | $0 | 0ms | B√°sica |
| **Produ√ß√£o B2C** | N√≠vel 1 | $50-100 | +500ms | 90% |
| **Casos Especiais** | N√≠vel 2 | $300-2000 | +10min | 100% |
| **Premium B2B** | N√≠vel 3 | $200-500 | +500ms | 100% |

## üöÄ **Estrat√©gia de Rollout**

### **Fase 1: Validation (Semanas 1-2)**
```bash
# Deploy sem APIs para validar estabilidade
export PERPLEXITY_API_KEY=""
export OPENAI_DEEP_KEY=""
```
- Monitorar logs: "Academic enrichment desabilitado"
- Validar que algoritmo funciona normalmente
- Baseline de performance sem enriquecimento

### **Fase 2: MVP (Semanas 3-4)**
```bash
# Ativar apenas Perplexity
export PERPLEXITY_API_KEY="pplx-xxx"
```
- Monitorar custos e lat√™ncia
- Avaliar melhoria no matching
- Cache hit rate ap√≥s aquecimento

### **Fase 3: Premium (M√™s 2+)**
```bash
# Adicionar Deep Research para casos premium
export OPENAI_DEEP_KEY="sk-xxx"
```
- Configurar flag de feature para casos B2B
- Monitorar uso do fallback
- ROI de casos high-value

## üîß **Configura√ß√£o Avan√ßada**

### **TTL do Cache**
```bash
# Padr√£o: 30 dias (720 horas)
export UNI_RANK_TTL_H="720"    # Universidades
export JOUR_RANK_TTL_H="720"   # Peri√≥dicos

# Deep Research timeouts (conforme spec oficial OpenAI)
export DEEP_POLL_SECS="10"     # intervalo entre polls (padr√£o 10s)
export DEEP_MAX_MIN="15"       # timeout m√°ximo (padr√£o 15 min)

# Desenvolvimento: cache curto
export UNI_RANK_TTL_H="1"      # 1 hora para testes
export JOUR_RANK_TTL_H="1"
```

### **Rate Limiting**
```python
# Em academic_prompt_templates.py
API_CONFIGS = {
    "perplexity": {
        "max_batch_size": 15,
        "rate_limit_per_min": 30,  # Ajustar conforme plano
        "timeout_seconds": 30
    }
}
```

### **Feature Flags por Cliente**
```python
# Para rollout gradual
def should_enrich_academic_data(user_id: str, case_type: str) -> bool:
    # Apenas casos B2B premium inicialmente
    if case_type == "CORPORATE" and user_id in PREMIUM_USERS:
        return True
    return False
```

## üìà **Monitoramento**

### **M√©tricas Chave**
```python
# KPIs para acompanhar
metrics = {
    "cache_hit_rate": redis_hits / total_requests,
    "api_coverage": resolved_items / total_items,
    "cost_per_case": monthly_cost / cases_processed,
    "latency_p95": percentile(response_times, 95),
    "quality_improvement": new_matching_score / old_matching_score
}
```

### **Alertas**
```bash
# CloudWatch/Grafana alerts
- API error rate > 5%
- Cache hit rate < 70%
- Latency P95 > 2s
- Monthly cost > $500
```

### **Logs Estruturados**
```json
{
  "event": "academic_enrichment_result",
  "universities_resolved": 5,
  "journals_resolved": 3,
  "cache_hits": 2,
  "api_calls": 1,
  "total_latency_ms": 450,
  "cost_estimate": 0.15
}
```

## ‚úÖ **Checklist de Deploy**

### **Pr√©-Deploy**
- [ ] Depend√™ncias instaladas (`aiohttp`, `aiolimiter`, `unidecode`)
- [ ] APIs configuradas conforme n√≠vel escolhido
- [ ] Redis funcionando
- [ ] Testes passando
- [ ] Monitoring configurado

### **Deploy**
- [ ] Deploy em staging primeiro
- [ ] Validar logs de fallback
- [ ] Testar casos com/sem cache
- [ ] Verificar m√©tricas de lat√™ncia
- [ ] Aprovar custos estimados

### **P√≥s-Deploy**
- [ ] Monitorar primeiras 24h
- [ ] Avaliar cache hit rate ap√≥s 48h
- [ ] Revisar custos semanais
- [ ] Coletar feedback de qualidade
- [ ] Planejar pr√≥ximo n√≠vel se necess√°rio

---

**üí° Recomenda√ß√£o:** Comece com **N√≠vel 1 (Apenas Perplexity)** para 95% dos casos. √â o melhor custo-benef√≠cio! 