#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Cluster Generation Job
======================

Job para geraÃ§Ã£o automatizada de clusters de casos e advogados usando:
- Coleta multi-fonte de dados (Escavador, Perplexity, Deep Research, LinkedIn)
- Embeddings hÃ­bridos com cascata Gemini â†’ OpenAI â†’ Local
- ClusterizaÃ§Ã£o consciente da origem: UMAP + HDBSCAN
- Rotulagem automÃ¡tica via LLM
- DetecÃ§Ã£o de clusters emergentes

ExecuÃ§Ã£o: A cada 6-12 horas via scheduler
"""

import asyncio
import logging
import numpy as np
import json
from datetime import datetime, timedelta
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass
from pathlib import Path

# Imports cientÃ­ficos
try:
    import umap
    import hdbscan
    from sklearn.metrics import silhouette_score
    from sklearn.preprocessing import StandardScaler
    CLUSTERING_AVAILABLE = True
except ImportError:
    logging.warning("âš ï¸ Bibliotecas de clustering nÃ£o instaladas: pip install umap-learn hdbscan scikit-learn")
    CLUSTERING_AVAILABLE = False

# Imports do projeto
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, text, and_, or_
from database import get_async_session

# ServiÃ§os
from services.cluster_data_collection_service import (
    ClusterDataCollectionService, 
    ConsolidatedClusterData, 
    ClusterDataType
)
from services.embedding_service import generate_embedding_with_provider
from services.cluster_labeling_service import ClusterLabelingService

# ðŸ†• ServiÃ§os para automaÃ§Ã£o de recomendaÃ§Ãµes e notificaÃ§Ãµes
from services.partnership_recommendation_service import PartnershipRecommendationService
from services.notify_service import send_notifications_to_lawyers


@dataclass
class ClusteringConfig:
    """ConfiguraÃ§Ãµes para clusterizaÃ§Ã£o."""
    # UMAP parameters
    umap_n_neighbors: int = 15
    umap_min_dist: float = 0.1
    umap_metric: str = 'cosine'
    umap_n_components: int = 50  # ReduÃ§Ã£o dimensional intermediÃ¡ria
    
    # HDBSCAN parameters  
    hdbscan_min_cluster_size: int = 5
    hdbscan_min_samples: int = 3
    hdbscan_cluster_selection_epsilon: float = 0.05
    hdbscan_metric: str = 'euclidean'
    
    # Embedding quality thresholds
    high_quality_providers: List[str] = None
    similarity_threshold: float = 0.7  # Para atribuir embeddings locais
    
    # Cluster validation
    min_silhouette_score: float = 0.3
    max_clusters: int = 100
    min_cluster_members: int = 3
    
    def __post_init__(self):
        if self.high_quality_providers is None:
            self.high_quality_providers = ['gemini', 'openai']


@dataclass
class ClusterResult:
    """Resultado de clusterizaÃ§Ã£o para uma entidade."""
    entity_id: str
    cluster_id: str
    confidence_score: float
    embedding_provider: str
    data_sources: Dict[str, bool]
    assignment_method: str  # 'hdbscan', 'similarity', 'outlier'


@dataclass
class ClusterMetadata:
    """Metadados de um cluster."""
    cluster_id: str
    cluster_type: str  # 'case' ou 'lawyer'
    total_members: int
    centroid: List[float]
    silhouette_score: float
    member_providers: Dict[str, int]  # {'gemini': 10, 'openai': 5, 'local': 2}
    data_quality_avg: float
    created_at: datetime


class ClusterGenerationJob:
    """Job principal para geraÃ§Ã£o de clusters."""
    
    def __init__(self, config: Optional[ClusteringConfig] = None):
        self.config = config or ClusteringConfig()
        self.logger = logging.getLogger(__name__)
        self.data_collector = ClusterDataCollectionService()
        
        # Validar disponibilidade de bibliotecas
        if not CLUSTERING_AVAILABLE:
            raise ImportError("Bibliotecas de clustering nÃ£o disponÃ­veis")
    
    async def run_clustering_pipeline(self, entity_type: ClusterDataType = None):
        """
        Pipeline completo de clusterizaÃ§Ã£o.
        
        Args:
            entity_type: Tipo especÃ­fico a processar (None = ambos)
        """
        start_time = datetime.now()
        self.logger.info(f"ðŸš€ Iniciando pipeline de clusterizaÃ§Ã£o - {start_time}")
        
        try:
            # Determinar tipos a processar
            types_to_process = [entity_type] if entity_type else [ClusterDataType.CASE, ClusterDataType.LAWYER]
            
            for cluster_type in types_to_process:
                self.logger.info(f"ðŸ“Š Processando clusterizaÃ§Ã£o de {cluster_type.value}s")
                
                # 1. Coletar dados consolidados
                consolidated_data = await self._collect_entities_data(cluster_type)
                if not consolidated_data:
                    self.logger.warning(f"âŒ Nenhum dado encontrado para {cluster_type.value}")
                    continue
                
                # 2. Gerar embeddings com rastreabilidade
                embeddings_data = await self._generate_embeddings_batch(consolidated_data)
                if not embeddings_data:
                    self.logger.warning(f"âŒ Falha na geraÃ§Ã£o de embeddings para {cluster_type.value}")
                    continue
                
                # 3. EstratÃ©gia de clusterizaÃ§Ã£o hÃ­brida
                cluster_results = await self._perform_hybrid_clustering(embeddings_data, cluster_type)
                
                # 4. Salvar resultados no banco
                await self._save_cluster_results(cluster_results, cluster_type)
                
                # 5. Detectar clusters emergentes
                await self._detect_emergent_clusters(cluster_results, cluster_type)
                
                # 6. Gerar rÃ³tulos automÃ¡ticos
                await self._generate_cluster_labels(cluster_results, cluster_type)
                
                self.logger.info(f"âœ… ClusterizaÃ§Ã£o de {cluster_type.value} concluÃ­da: {len(cluster_results)} entidades processadas")
            
            duration = datetime.now() - start_time
            self.logger.info(f"ðŸŽ‰ Pipeline completo concluÃ­do em {duration.total_seconds():.1f}s")
            
        except Exception as e:
            self.logger.error(f"âŒ Erro no pipeline de clusterizaÃ§Ã£o: {e}")
            raise
    
    async def _collect_entities_data(self, entity_type: ClusterDataType) -> List[ConsolidatedClusterData]:
        """Coleta dados consolidados de entidades para clusterizaÃ§Ã£o."""
        
        consolidated_data = []
        
        try:
            async with get_async_session() as db:
                # Buscar entidades que precisam de clusterizaÃ§Ã£o
                if entity_type == ClusterDataType.CASE:
                    # Buscar casos sem cluster ou com cluster antigo
                    query = text("""
                        SELECT DISTINCT c.id, c.title, c.description
                        FROM cases c
                        LEFT JOIN case_clusters cc ON c.id = cc.case_id 
                        WHERE cc.case_id IS NULL 
                           OR cc.updated_at < NOW() - INTERVAL '24 hours'
                        LIMIT 500
                    """)
                    
                    result = await db.execute(query)
                    entities = result.fetchall()
                    
                    # Coletar dados para cada caso
                    self.logger.info(f"ðŸ“¦ Coletando dados de {len(entities)} casos")
                    
                    for entity in entities:
                        case_data = await self.data_collector.collect_case_data_for_clustering(str(entity.id))
                        if case_data:
                            consolidated_data.append(case_data)
                
                elif entity_type == ClusterDataType.LAWYER:
                    # Buscar advogados sem cluster ou com cluster antigo
                    query = text("""
                        SELECT DISTINCT l.id, l.oab_number, l.name
                        FROM lawyers l
                        LEFT JOIN lawyer_clusters lc ON l.id = lc.lawyer_id
                        WHERE lc.lawyer_id IS NULL 
                           OR lc.updated_at < NOW() - INTERVAL '24 hours'
                        LIMIT 500
                    """)
                    
                    result = await db.execute(query)
                    entities = result.fetchall()
                    
                    # Coletar dados para cada advogado
                    self.logger.info(f"ðŸ“¦ Coletando dados de {len(entities)} advogados")
                    
                    for entity in entities:
                        lawyer_data = await self.data_collector.collect_lawyer_data_for_clustering(
                            str(entity.id), 
                            entity.oab_number
                        )
                        if lawyer_data:
                            consolidated_data.append(lawyer_data)
        
        except Exception as e:
            self.logger.error(f"âŒ Erro ao coletar dados de entidades: {e}")
        
        self.logger.info(f"âœ… Coletados dados de {len(consolidated_data)} entidades vÃ¡lidas")
        return consolidated_data
    
    async def _generate_embeddings_batch(self, consolidated_data: List[ConsolidatedClusterData]) -> List[Dict[str, Any]]:
        """Gera embeddings em batch com rastreabilidade."""
        
        embeddings_data = []
        
        self.logger.info(f"ðŸ§  Gerando embeddings para {len(consolidated_data)} entidades")
        
        # Processar em batches para otimizar performance
        batch_size = 10
        for i in range(0, len(consolidated_data), batch_size):
            batch = consolidated_data[i:i + batch_size]
            
            # Gerar embeddings do batch em paralelo
            batch_tasks = [
                self._generate_single_embedding(data)
                for data in batch
            ]
            
            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
            
            # Processar resultados
            for result in batch_results:
                if isinstance(result, Exception):
                    self.logger.error(f"Erro na geraÃ§Ã£o de embedding: {result}")
                    continue
                
                if result:
                    embeddings_data.append(result)
            
            self.logger.debug(f"ðŸ“Š Batch {i//batch_size + 1}/{(len(consolidated_data) + batch_size - 1)//batch_size} concluÃ­do")
        
        self.logger.info(f"âœ… Embeddings gerados: {len(embeddings_data)}/{len(consolidated_data)}")
        return embeddings_data
    
    async def _generate_single_embedding(self, data: ConsolidatedClusterData) -> Optional[Dict[str, Any]]:
        """Gera embedding para uma entidade com rastreabilidade."""
        
        try:
            # Gerar embedding com rastreabilidade de provider
            embedding_vector, provider = await generate_embedding_with_provider(
                data.consolidated_text,
                allow_local_fallback=True
            )
            
            return {
                'entity_id': data.entity_id,
                'entity_type': data.entity_type.value,
                'embedding': embedding_vector,
                'provider': provider,
                'data_sources': data.data_sources,
                'data_quality_score': data.data_quality_score,
                'consolidated_text': data.consolidated_text,
                'text_length': len(data.consolidated_text),
                'metadata': data.metadata
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Erro ao gerar embedding para {data.entity_id}: {e}")
            return None
    
    async def _perform_hybrid_clustering(self, embeddings_data: List[Dict], entity_type: ClusterDataType) -> List[ClusterResult]:
        """
        Executa clusterizaÃ§Ã£o hÃ­brida consciente da origem dos embeddings.
        
        EstratÃ©gia:
        1. Separar embeddings por qualidade (high-quality vs local)
        2. Clusterizar primeiro os de alta qualidade com UMAP+HDBSCAN
        3. Atribuir embeddings locais aos clusters via similaridade
        4. Aplicar pÃ³s-processamento e validaÃ§Ã£o
        """
        
        self.logger.info(f"ðŸŽ¯ Executando clusterizaÃ§Ã£o hÃ­brida para {len(embeddings_data)} embeddings")
        
        # 1. Separar por qualidade do provider
        high_quality_data = [
            item for item in embeddings_data 
            if item['provider'] in self.config.high_quality_providers
        ]
        local_data = [
            item for item in embeddings_data 
            if item['provider'] not in self.config.high_quality_providers
        ]
        
        self.logger.info(f"ðŸ“Š Dados de alta qualidade: {len(high_quality_data)}, Locais: {len(local_data)}")
        
        cluster_results = []
        
        if not high_quality_data:
            self.logger.warning("âŒ Nenhum embedding de alta qualidade disponÃ­vel")
            return cluster_results
        
        # 2. ClusterizaÃ§Ã£o principal com dados de alta qualidade
        hq_embeddings = np.array([item['embedding'] for item in high_quality_data])
        
        self.logger.info("ðŸ”„ Aplicando UMAP para reduÃ§Ã£o dimensional...")
        
        # UMAP para reduÃ§Ã£o dimensional otimizada
        umap_reducer = umap.UMAP(
            n_neighbors=min(self.config.umap_n_neighbors, len(high_quality_data) - 1),
            min_dist=self.config.umap_min_dist,
            metric=self.config.umap_metric,
            n_components=min(self.config.umap_n_components, len(high_quality_data) - 1),
            random_state=42
        )
        
        hq_embeddings_umap = umap_reducer.fit_transform(hq_embeddings)
        
        self.logger.info("ðŸŽ¯ Aplicando HDBSCAN para clusterizaÃ§Ã£o...")
        
        # HDBSCAN na representaÃ§Ã£o UMAP
        clusterer = hdbscan.HDBSCAN(
            min_cluster_size=max(self.config.hdbscan_min_cluster_size, 3),
            min_samples=self.config.hdbscan_min_samples,
            cluster_selection_epsilon=self.config.hdbscan_cluster_selection_epsilon,
            metric=self.config.hdbscan_metric
        )
        
        cluster_labels = clusterer.fit_predict(hq_embeddings_umap)
        
        # 3. Processar resultados de alta qualidade
        unique_clusters = set(cluster_labels)
        n_clusters = len(unique_clusters) - (1 if -1 in unique_clusters else 0)
        
        self.logger.info(f"âœ… HDBSCAN encontrou {n_clusters} clusters (outliers: {sum(1 for x in cluster_labels if x == -1)})")
        
        # Calcular centroides dos clusters para atribuiÃ§Ã£o posterior
        cluster_centroids = {}
        for cluster_id in unique_clusters:
            if cluster_id != -1:  # Ignorar outliers
                mask = cluster_labels == cluster_id
                centroid = np.mean(hq_embeddings[mask], axis=0)
                cluster_centroids[cluster_id] = centroid
        
        # 4. Adicionar resultados de alta qualidade
        for i, item in enumerate(high_quality_data):
            cluster_id = cluster_labels[i]
            
            # Calcular confidence score baseado na distÃ¢ncia ao centrÃ³ide
            if cluster_id != -1 and cluster_id in cluster_centroids:
                embedding_vec = np.array(item['embedding'])
                centroid = cluster_centroids[cluster_id]
                similarity = np.dot(embedding_vec, centroid) / (
                    np.linalg.norm(embedding_vec) * np.linalg.norm(centroid)
                )
                confidence_score = max(0.1, similarity)
            else:
                confidence_score = 0.1  # Outlier
            
            cluster_results.append(ClusterResult(
                entity_id=item['entity_id'],
                cluster_id=f"{entity_type.value}_cluster_{cluster_id}",
                confidence_score=confidence_score,
                embedding_provider=item['provider'],
                data_sources=item['data_sources'],
                assignment_method='hdbscan'
            ))
        
        # 5. Atribuir embeddings locais aos clusters via similaridade
        if local_data and cluster_centroids:
            self.logger.info(f"ðŸ”— Atribuindo {len(local_data)} embeddings locais aos clusters")
            
            for item in local_data:
                embedding_vec = np.array(item['embedding'])
                best_cluster_id = -1
                best_similarity = -1
                
                # Encontrar cluster mais similar
                for cluster_id, centroid in cluster_centroids.items():
                    similarity = np.dot(embedding_vec, centroid) / (
                        np.linalg.norm(embedding_vec) * np.linalg.norm(centroid)
                    )
                    
                    if similarity > best_similarity and similarity > self.config.similarity_threshold:
                        best_similarity = similarity
                        best_cluster_id = cluster_id
                
                cluster_results.append(ClusterResult(
                    entity_id=item['entity_id'],
                    cluster_id=f"{entity_type.value}_cluster_{best_cluster_id}",
                    confidence_score=max(0.05, best_similarity),
                    embedding_provider=item['provider'],
                    data_sources=item['data_sources'],
                    assignment_method='similarity' if best_cluster_id != -1 else 'outlier'
                ))
        
        # 6. ValidaÃ§Ã£o e mÃ©tricas de qualidade
        await self._validate_clustering_quality(cluster_results, hq_embeddings, cluster_labels)
        
        # 7. AnÃ¡lise de qualidade detalhada dos clusters gerados
        await self._analyze_generated_clusters_quality(cluster_results, entity_type)
        
        self.logger.info(f"âœ… ClusterizaÃ§Ã£o hÃ­brida concluÃ­da: {len(cluster_results)} entidades atribuÃ­das")
        
        return cluster_results
    
    async def _validate_clustering_quality(self, cluster_results: List[ClusterResult], embeddings: np.ndarray, labels: np.ndarray):
        """Valida qualidade da clusterizaÃ§Ã£o."""
        
        try:
            # Calcular silhouette score apenas para clusters vÃ¡lidos (nÃ£o outliers)
            valid_mask = labels != -1
            if np.sum(valid_mask) >= 2:
                silhouette_avg = silhouette_score(embeddings[valid_mask], labels[valid_mask])
                self.logger.info(f"ðŸ“Š Silhouette Score: {silhouette_avg:.3f}")
                
                if silhouette_avg < self.config.min_silhouette_score:
                    self.logger.warning(f"âš ï¸ Silhouette score baixo: {silhouette_avg:.3f} < {self.config.min_silhouette_score}")
            
            # EstatÃ­sticas dos clusters
            cluster_sizes = {}
            for result in cluster_results:
                cluster_id = result.cluster_id
                cluster_sizes[cluster_id] = cluster_sizes.get(cluster_id, 0) + 1
            
            valid_clusters = {k: v for k, v in cluster_sizes.items() if not k.endswith('_-1') and v >= self.config.min_cluster_members}
            
            self.logger.info(f"ðŸ“ˆ Clusters vÃ¡lidos: {len(valid_clusters)}")
            self.logger.info(f"ðŸ“ˆ Cluster sizes: {dict(sorted(valid_clusters.items(), key=lambda x: x[1], reverse=True)[:5])}")
            
        except Exception as e:
            self.logger.error(f"âŒ Erro na validaÃ§Ã£o de qualidade: {e}")
    
    async def _analyze_generated_clusters_quality(self, cluster_results: List[ClusterResult], entity_type: ClusterDataType):
        """AnÃ¡lise de qualidade dos clusters gerados."""
        
        try:
            from services.cluster_quality_metrics_service import create_quality_metrics_service
            
            # Agrupar por cluster
            cluster_groups = {}
            for result in cluster_results:
                cluster_id = result.cluster_id
                if cluster_id not in cluster_groups:
                    cluster_groups[cluster_id] = []
                cluster_groups[cluster_id].append(result)
            
            self.logger.info(f"ðŸ” Analisando qualidade de {len(cluster_groups)} clusters gerados")
            
            # Analisar qualidade dos clusters com mais de 5 membros
            quality_analyzed = 0
            async with get_async_session() as db:
                quality_service = create_quality_metrics_service(db)
                
                for cluster_id, members in cluster_groups.items():
                    if cluster_id.endswith('_-1'):  # Ignorar outliers
                        continue
                    
                    if len(members) >= 5:  # SÃ³ analisar clusters com tamanho mÃ­nimo
                        try:
                            quality_report = await quality_service.analyze_cluster_quality(
                                cluster_id, include_detailed_analysis=False
                            )
                            
                            if quality_report:
                                quality_analyzed += 1
                                
                                # Log da qualidade
                                self.logger.info(
                                    f"ðŸ“Š Cluster {cluster_id}: "
                                    f"Qualidade={quality_report.overall_quality_score:.3f} "
                                    f"({quality_report.quality_level.name}), "
                                    f"Silhouette={quality_report.silhouette_analysis.silhouette_avg:.3f}"
                                )
                                
                                # Alertar sobre clusters de baixa qualidade
                                if quality_report.overall_quality_score < 0.4:
                                    self.logger.warning(
                                        f"âš ï¸ Cluster {cluster_id} tem baixa qualidade: "
                                        f"{quality_report.overall_quality_score:.3f}. "
                                        f"RecomendaÃ§Ãµes: {'; '.join(quality_report.actionable_insights[:2])}"
                                    )
                                
                        except Exception as e:
                            self.logger.error(f"âŒ Erro na anÃ¡lise de qualidade do cluster {cluster_id}: {e}")
                            continue
            
            self.logger.info(f"âœ… AnÃ¡lise de qualidade concluÃ­da: {quality_analyzed} clusters analisados")
            
        except Exception as e:
            self.logger.error(f"âŒ Erro na anÃ¡lise de qualidade dos clusters gerados: {e}")
            # NÃ£o falhar o pipeline por causa deste erro
    
    async def _save_cluster_results(self, cluster_results: List[ClusterResult], entity_type: ClusterDataType):
        """Salva resultados da clusterizaÃ§Ã£o no banco."""
        
        if not cluster_results:
            return
        
        try:
            async with get_async_session() as db:
                table_name = f"{entity_type.value}_clusters"
                
                self.logger.info(f"ðŸ’¾ Salvando {len(cluster_results)} resultados em {table_name}")
                
                # Limpar clusters antigos
                await db.execute(text(f"DELETE FROM {table_name} WHERE updated_at < NOW() - INTERVAL '1 day'"))
                
                # Inserir novos resultados
                for result in cluster_results:
                    insert_query = text(f"""
                        INSERT INTO {table_name} (
                            {entity_type.value}_id, cluster_id, confidence_score, 
                            assigned_method, created_at, updated_at
                        ) VALUES (
                            :entity_id, :cluster_id, :confidence_score,
                            :assignment_method, NOW(), NOW()
                        )
                        ON CONFLICT ({entity_type.value}_id) 
                        DO UPDATE SET 
                            cluster_id = :cluster_id,
                            confidence_score = :confidence_score,
                            assigned_method = :assignment_method,
                            updated_at = NOW()
                    """)
                    
                    await db.execute(insert_query, {
                        'entity_id': result.entity_id,
                        'cluster_id': result.cluster_id,
                        'confidence_score': result.confidence_score,
                        'assignment_method': result.assignment_method
                    })
                
                await db.commit()
                self.logger.info(f"âœ… Resultados salvos com sucesso em {table_name}")
                
        except Exception as e:
            self.logger.error(f"âŒ Erro ao salvar resultados: {e}")
            await db.rollback()
    
    async def _detect_emergent_clusters(self, cluster_results: List[ClusterResult], entity_type: ClusterDataType):
        """Detecta clusters emergentes baseado em momentum e crescimento temporal."""
        
        try:
            from services.cluster_momentum_service import create_momentum_service
            
            # Agrupar por cluster
            cluster_groups = {}
            for result in cluster_results:
                cluster_id = result.cluster_id
                if cluster_id not in cluster_groups:
                    cluster_groups[cluster_id] = []
                cluster_groups[cluster_id].append(result)
            
            self.logger.info(f"ðŸš€ Analisando {len(cluster_groups)} clusters para detecÃ§Ã£o de emergentes")
            
            # Usar o serviÃ§o de momentum para detecÃ§Ã£o avanÃ§ada
            async with get_async_session() as db:
                momentum_service = create_momentum_service(db)
                
                # 1. Atualizar histÃ³rico de momentum para todos os clusters
                for cluster_id, members in cluster_groups.items():
                    if cluster_id.endswith('_-1'):  # Ignorar outliers
                        continue
                    
                    total_members = len(members)
                    await momentum_service.update_cluster_momentum_history(cluster_id, total_members)
                
                # 2. Detectar clusters emergentes usando algoritmo de momentum
                emergent_alerts = await momentum_service.detect_emergent_clusters(entity_type.value)
                
                # 3. Processar alertas e atualizar metadata
                if emergent_alerts:
                    self.logger.info(f"ðŸš€ Detectados {len(emergent_alerts)} clusters emergentes com anÃ¡lise de momentum")
                    
                    for alert in emergent_alerts:
                        self.logger.info(
                            f"ðŸ“Š Cluster emergente: {alert.cluster_label} "
                            f"(momentum: {alert.momentum_score:.3f}, growth: {alert.growth_rate:.3f})"
                        )
                        
                        # Calcular mÃ©tricas detalhadas
                        momentum_metrics = await momentum_service.calculate_cluster_momentum(alert.cluster_id)
                        
                        if momentum_metrics:
                            # Marcar como emergente no banco
                            success = await momentum_service.mark_cluster_as_emergent(alert.cluster_id, momentum_metrics)
                            
                            if success:
                                self.logger.info(f"âœ… Cluster {alert.cluster_id} marcado como emergente")
                            
                            # Salvar mÃ©tricas detalhadas no metadata
                            update_metadata_query = text("""
                                INSERT INTO cluster_metadata (
                                    cluster_id, cluster_type, total_items, 
                                    is_emergent, emergent_since, momentum_score,
                                    description, last_updated
                                ) VALUES (
                                    :cluster_id, :cluster_type, :total_items,
                                    true, NOW(), :momentum_score, :description, NOW()
                                )
                                ON CONFLICT (cluster_id)
                                DO UPDATE SET
                                    total_items = :total_items,
                                    is_emergent = true,
                                    emergent_since = COALESCE(cluster_metadata.emergent_since, NOW()),
                                    momentum_score = :momentum_score,
                                    description = :description,
                                    last_updated = NOW()
                            """)
                            
                            await db.execute(update_metadata_query, {
                                'cluster_id': alert.cluster_id,
                                'cluster_type': entity_type.value,
                                'total_items': cluster_groups.get(alert.cluster_id, []),
                                'momentum_score': momentum_metrics.current_momentum,
                                'description': alert.market_opportunity
                            })
                
                # 4. Fallback para clusters que nÃ£o tÃªm histÃ³rico suficiente (critÃ©rios bÃ¡sicos)
                basic_emergent_clusters = []
                
                for cluster_id, members in cluster_groups.items():
                    if cluster_id.endswith('_-1'):
                        continue
                    
                    # Verificar se jÃ¡ foi analisado pelo momentum service
                    already_detected = any(alert.cluster_id == cluster_id for alert in emergent_alerts)
                    if already_detected:
                        continue
                    
                    # Aplicar critÃ©rios bÃ¡sicos para clusters novos
                    total_members = len(members)
                    avg_confidence = sum(m.confidence_score for m in members) / total_members
                    
                    is_emergent_basic = (
                        total_members >= 5 and 
                        avg_confidence > 0.6 and
                        total_members <= 20  # Clusters pequenos mas significativos
                    )
                    
                    if is_emergent_basic:
                        basic_emergent_clusters.append({
                            'cluster_id': cluster_id,
                            'member_count': total_members,
                            'avg_confidence': avg_confidence,
                            'detected_at': datetime.now()
                        })
                
                # 5. Salvar clusters emergentes bÃ¡sicos
                if basic_emergent_clusters:
                    self.logger.info(f"ðŸ“ˆ Detectados {len(basic_emergent_clusters)} clusters emergentes (critÃ©rios bÃ¡sicos)")
                    
                    for cluster_info in basic_emergent_clusters:
                        basic_update_query = text("""
                            INSERT INTO cluster_metadata (
                                cluster_id, cluster_type, total_items, 
                                is_emergent, emergent_since, momentum_score, last_updated
                            ) VALUES (
                                :cluster_id, :cluster_type, :total_items,
                                true, NOW(), 0.5, NOW()
                            )
                            ON CONFLICT (cluster_id)
                            DO UPDATE SET
                                total_items = :total_items,
                                is_emergent = true,
                                emergent_since = COALESCE(cluster_metadata.emergent_since, NOW()),
                                momentum_score = GREATEST(cluster_metadata.momentum_score, 0.5),
                                last_updated = NOW()
                        """)
                        
                        await db.execute(basic_update_query, {
                            'cluster_id': cluster_info['cluster_id'],
                            'cluster_type': entity_type.value,
                            'total_items': cluster_info['member_count']
                        })
                
                await db.commit()
                self.logger.info("âœ… DetecÃ§Ã£o de clusters emergentes concluÃ­da com anÃ¡lise de momentum")
                
                # ðŸ†• NOVA FUNCIONALIDADE: Disparar recomendaÃ§Ãµes automÃ¡ticas
                if entity_type == ClusterDataType.LAWYER and (emergent_alerts or basic_emergent_clusters):
                    await self._trigger_partnership_recommendations(emergent_alerts, basic_emergent_clusters, db)
            
        except Exception as e:
            self.logger.error(f"âŒ Erro na detecÃ§Ã£o de clusters emergentes: {e}")
            # NÃ£o falhar o pipeline inteiro por causa deste erro
    
    async def _generate_cluster_labels(self, cluster_results: List[ClusterResult], entity_type: ClusterDataType):
        """Gera rÃ³tulos automÃ¡ticos para clusters via LLM."""
        
        try:
            async with get_async_session() as db:
                labeling_service = ClusterLabelingService(db)
                await labeling_service.label_all_clusters(
                    entity_type=entity_type.value,
                    model="gpt-4o",
                    n_samples=5
                )
                
                self.logger.info(f"ðŸ·ï¸ RÃ³tulos gerados para clusters de {entity_type.value}")
                
        except Exception as e:
            self.logger.error(f"âŒ Erro na geraÃ§Ã£o de rÃ³tulos: {e}")
    
    async def _trigger_partnership_recommendations(self, emergent_alerts: List, basic_emergent_clusters: List, db: AsyncSession):
        """
        ðŸ†• Dispara recomendaÃ§Ãµes de parceria automÃ¡ticas baseadas em clusters emergentes de advogados.
        """
        
        self.logger.info("ðŸ¤ Iniciando geraÃ§Ã£o automÃ¡tica de recomendaÃ§Ãµes de parceria")
        
        try:
            # Coletar IDs de advogados dos clusters emergentes
            affected_lawyer_ids = set()
            cluster_contexts = {}
            
            # Processar alertas do momentum service
            for alert in emergent_alerts:
                cluster_members = await self._get_cluster_members(alert.cluster_id, db)
                for member in cluster_members:
                    affected_lawyer_ids.add(member['lawyer_id'])
                    cluster_contexts[member['lawyer_id']] = {
                        'cluster_label': alert.cluster_label,
                        'market_opportunity': alert.market_opportunity,
                        'momentum_score': alert.momentum_score,
                        'growth_rate': alert.growth_rate
                    }
            
            # Processar clusters emergentes bÃ¡sicos
            for cluster_info in basic_emergent_clusters:
                cluster_members = await self._get_cluster_members(cluster_info['cluster_id'], db)
                for member in cluster_members:
                    affected_lawyer_ids.add(member['lawyer_id'])
                    if member['lawyer_id'] not in cluster_contexts:
                        cluster_contexts[member['lawyer_id']] = {
                            'cluster_label': f"Cluster {cluster_info['cluster_id'][:8]}",
                            'market_opportunity': "Nova Ã¡rea de oportunidade detectada",
                            'momentum_score': cluster_info['avg_confidence'],
                            'growth_rate': 0.0
                        }
            
            if not affected_lawyer_ids:
                self.logger.info("Nenhum advogado encontrado nos clusters emergentes")
                return
            
            self.logger.info(f"ðŸ“Š Processando recomendaÃ§Ãµes para {len(affected_lawyer_ids)} advogados de clusters emergentes")
            
            # Inicializar serviÃ§o de recomendaÃ§Ãµes
            partnership_service = PartnershipRecommendationService(db)
            
            # Gerar recomendaÃ§Ãµes para cada advogado afetado
            recommendations_generated = []
            
            for lawyer_id in affected_lawyer_ids:
                try:
                    # Gerar recomendaÃ§Ãµes hÃ­bridas (incluindo busca externa)
                    recommendations = await partnership_service.get_recommendations(
                        lawyer_id=lawyer_id,
                        limit=5,
                        expand_search=True  # Usar busca hÃ­brida para maximizar oportunidades
                    )
                    
                    if recommendations:
                        context = cluster_contexts.get(lawyer_id, {})
                        recommendations_generated.append({
                            'lawyer_id': lawyer_id,
                            'recommendations_count': len(recommendations),
                            'cluster_context': context
                        })
                        
                        self.logger.info(
                            f"âœ… {len(recommendations)} recomendaÃ§Ãµes geradas para advogado {lawyer_id} "
                            f"(cluster: {context.get('cluster_label', 'N/A')})"
                        )
                    
                except Exception as e:
                    self.logger.error(f"âŒ Erro ao gerar recomendaÃ§Ãµes para advogado {lawyer_id}: {e}")
                    continue
            
            # Disparar notificaÃ§Ãµes para advogados com novas recomendaÃ§Ãµes
            if recommendations_generated:
                await self._send_partnership_notifications(recommendations_generated)
                
                self.logger.info(
                    f"ðŸŽ‰ AutomaÃ§Ã£o concluÃ­da: {len(recommendations_generated)} advogados receberam "
                    f"novas recomendaÃ§Ãµes de parceria baseadas em clusters emergentes"
                )
            
        except Exception as e:
            self.logger.error(f"âŒ Erro na automaÃ§Ã£o de recomendaÃ§Ãµes de parceria: {e}")
    
    async def _get_cluster_members(self, cluster_id: str, db: AsyncSession) -> List[Dict[str, Any]]:
        """ObtÃ©m membros de um cluster especÃ­fico."""
        
        try:
            # Query para obter advogados do cluster
            query = text("""
                SELECT 
                    le.entity_id as lawyer_id,
                    le.confidence_score,
                    le.embedding_provider,
                    p.full_name,
                    p.email
                FROM lawyer_embeddings le
                LEFT JOIN profiles p ON p.id = le.entity_id
                WHERE le.cluster_id = :cluster_id
                AND le.entity_id IS NOT NULL
                ORDER BY le.confidence_score DESC
            """)
            
            result = await db.execute(query, {'cluster_id': cluster_id})
            members = [dict(row._mapping) for row in result.fetchall()]
            
            return members
            
        except Exception as e:
            self.logger.error(f"âŒ Erro ao buscar membros do cluster {cluster_id}: {e}")
            return []
    
    async def _send_partnership_notifications(self, recommendations_data: List[Dict[str, Any]]):
        """
        ðŸ†• Envia notificaÃ§Ãµes push sobre novas oportunidades de parceria.
        """
        
        self.logger.info("ðŸ“± Enviando notificaÃ§Ãµes de parceria para advogados")
        
        try:
            # Preparar lista de IDs para notificaÃ§Ã£o
            lawyer_ids = [data['lawyer_id'] for data in recommendations_data]
            
            # Preparar payload da notificaÃ§Ã£o
            notification_payload = {
                "headline": "ðŸ¤ Novas Oportunidades de Parceria Detectadas",
                "summary": "Descobrimos novas oportunidades de parceria estratÃ©gica baseadas em anÃ¡lise de mercado emergente. Confira as recomendaÃ§Ãµes personalizadas.",
                "data": {
                    "type": "partnership_opportunities",
                    "action": "open_partnerships_screen",
                    "source": "cluster_analysis",
                    "timestamp": datetime.now().isoformat(),
                    "stats": {
                        "total_lawyers_notified": len(lawyer_ids),
                        "avg_recommendations_per_lawyer": sum(d['recommendations_count'] for d in recommendations_data) / len(recommendations_data) if recommendations_data else 0
                    }
                }
            }
            
            # Enviar notificaÃ§Ãµes via serviÃ§o existente
            await send_notifications_to_lawyers(lawyer_ids, notification_payload)
            
            # Log detalhado
            for data in recommendations_data[:5]:  # Mostrar apenas os primeiros 5 para nÃ£o poluir logs
                context = data['cluster_context']
                self.logger.info(
                    f"ðŸ“± NotificaÃ§Ã£o enviada: Advogado {data['lawyer_id']} â†’ "
                    f"{data['recommendations_count']} recomendaÃ§Ãµes â†’ "
                    f"Cluster: {context.get('cluster_label', 'N/A')}"
                )
            
            if len(recommendations_data) > 5:
                self.logger.info(f"ðŸ“± ... e mais {len(recommendations_data) - 5} advogados notificados")
            
            self.logger.info(f"âœ… NotificaÃ§Ãµes enviadas para {len(lawyer_ids)} advogados sobre oportunidades de parceria")
            
        except Exception as e:
            self.logger.error(f"âŒ Erro ao enviar notificaÃ§Ãµes de parceria: {e}")


# FunÃ§Ã£o principal para execuÃ§Ã£o via scheduler
async def run_cluster_generation(entity_type: str = None):
    """
    FunÃ§Ã£o principal para execuÃ§Ã£o do job de clusterizaÃ§Ã£o.
    
    Args:
        entity_type: 'case', 'lawyer' ou None (ambos)
    """
    
    try:
        # ConfiguraÃ§Ã£o otimizada
        config = ClusteringConfig(
            umap_n_neighbors=10,
            umap_min_dist=0.05,
            hdbscan_min_cluster_size=4,
            hdbscan_min_samples=2,
            similarity_threshold=0.65
        )
        
        # Determinar tipo
        cluster_type = None
        if entity_type:
            cluster_type = ClusterDataType.CASE if entity_type == 'case' else ClusterDataType.LAWYER
        
        # Executar job
        job = ClusterGenerationJob(config)
        await job.run_clustering_pipeline(cluster_type)
        
        return {"status": "success", "message": "ClusterizaÃ§Ã£o concluÃ­da"}
        
    except Exception as e:
        logging.error(f"âŒ Erro no job de clusterizaÃ§Ã£o: {e}")
        return {"status": "error", "message": str(e)}


if __name__ == "__main__":
    # Teste local
    import sys
    
    entity_type = sys.argv[1] if len(sys.argv) > 1 else None
    result = asyncio.run(run_cluster_generation(entity_type))
    print(f"Resultado: {result}") 