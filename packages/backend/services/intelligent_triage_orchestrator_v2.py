#!/usr/bin/env python3
"""
Intelligent Triage Orchestrator V2 - LangGraph 0.4
==================================================

Vers√£o V2 migrada para usar LangGraph 0.4 com:
- Workflow declarativo com nodes e edges
- Checkpointing autom√°tico com MemorySaver
- Interrupts nativos para pausas inteligentes
- Estado centralizado e versionado
- Visualiza√ß√£o autom√°tica do fluxo
- INTEGRA√á√ÉO REAL com todos os servi√ßos

Mant√©m 100% compatibilidade com V1 para orquestra√ß√£o de triagem inteligente.
"""

import asyncio
import json
import logging
import time
from typing import TypedDict, Annotated, List, Optional, Dict, Any
from dataclasses import dataclass
from datetime import datetime
import operator
import os # Added for os.getenv

# LangGraph 0.4 imports
try:
    from langgraph.graph import StateGraph, END
    from langgraph.checkpoint.memory import MemorySaver
    from langchain_core.runnables import RunnableLambda
    LANGGRAPH_AVAILABLE = True
except ImportError:
    LANGGRAPH_AVAILABLE = False
    print("‚ö†Ô∏è LangGraph n√£o dispon√≠vel - usando simula√ß√£o")

# Imports dos servi√ßos reais
try:
    from services.intelligent_interviewer_service import intelligent_interviewer_service
    from services.triage_service import triage_service
    from services.lex9000_integration_service import lex9000_integration_service
    from services.conversation_state_manager import conversation_state_manager
    from services.redis_service import redis_service
    from services.notify_service import send_notification_to_client
    from services.match_service import find_and_notify_matches, MatchRequest
    from services.embedding_orchestrator import generate_embedding
    from utils.case_type_mapper import map_area_to_case_type
    
    # NOVO: LangChain-Grok para agentes
    try:
        from langchain_xai import ChatXAI
        from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
        LANGCHAIN_GROK_AVAILABLE = True
    except ImportError:
        LANGCHAIN_GROK_AVAILABLE = False
        print("‚ö†Ô∏è LangChain-Grok n√£o dispon√≠vel")
    
    SERVICES_AVAILABLE = True
except ImportError as e:
    print(f"‚ö†Ô∏è Alguns servi√ßos n√£o dispon√≠veis: {e}")
    SERVICES_AVAILABLE = False
    LANGCHAIN_GROK_AVAILABLE = False

logger = logging.getLogger(__name__)

# ===== DEFINI√á√ÉO DO ESTADO =====

class TriageState(TypedDict):
    """Estado centralizado do workflow de triagem inteligente."""
    
    # Identificadores
    user_id: str
    case_id: Optional[str]
    thread_id: Optional[str]
    
    # Dados de entrada
    conversation_data: Dict[str, Any]  # Removido Annotated para evitar erro de merge
    user_preferences: Dict[str, Any]
    
    # An√°lises intermedi√°rias
    complexity_level: str
    basic_triage_result: Dict[str, Any]
    lex_analysis: Optional[Dict[str, Any]]
    
    # Matching e enhancement
    initial_matches: List[Dict[str, Any]]
    enhanced_matches: List[Dict[str, Any]]
    match_explanations: List[str]
    
    # Estado de execu√ß√£o
    current_step: str
    error_context: Optional[str]
    notifications_sent: bool
    workflow_paused: bool
    
    # M√©tricas
    processing_start_time: float
    step_durations: List[Dict[str, Any]]  # Removido Annotated para evitar erro de merge

@dataclass
class TriageResult:
    """Resultado final da triagem V2."""
    success: bool
    case_id: Optional[str]
    triage_result: Dict[str, Any]
    lex_analysis: Optional[Dict[str, Any]]
    matches: List[Dict[str, Any]]
    explanations: List[str]
    error: Optional[str]
    processing_summary: Dict[str, Any]
    workflow_visualization: Optional[str]

class IntelligentTriageOrchestratorV2:
    """
    Orquestrador V2 para triagem inteligente usando LangGraph 0.4.
    
    Transforma a l√≥gica imperativa do V1 em um workflow declarativo e visual,
    com estado centralizado, checkpointing autom√°tico e interrupts nativos.
    
    INTEGRA√á√ÉO REAL com todos os servi√ßos:
    - IntelligentInterviewerService (Claude Sonnet)
    - TriageService (Llama 4 Scout + Claude)
    - LEX9000IntegrationService (Grok 4 via OpenRouter)
    - Redis para estado persistente
    - MatchService para matching de advogados
    - NotifyService para notifica√ß√µes
    
    Mant√©m 100% compatibilidade com V1.
    """
    
    def __init__(self):
        self.logger = logging.getLogger(f"{self.__class__.__name__}")
        
        # Inicializar servi√ßos reais
        self.services = self._initialize_services()
        
        # Inicializar LangGraph workflow se dispon√≠vel
        if LANGGRAPH_AVAILABLE:
            self.workflow = self._build_langgraph_workflow()
            self.compiled_workflow = self._compile_workflow()
        else:
            self.workflow = None
            self.compiled_workflow = None
    
    def _initialize_services(self) -> Dict[str, Any]:
        """Inicializa servi√ßos dependentes REAIS com l√≥gica espec√≠fica de modelos."""
        services = {}
        
        if SERVICES_AVAILABLE:
            try:
                services["interviewer"] = intelligent_interviewer_service
                self.logger.info("‚úÖ IntelligentInterviewerService inicializado (Claude Sonnet)")
            except Exception as e:
                self.logger.warning(f"‚ùå IntelligentInterviewerService falhou: {e}")
                services["interviewer"] = None
            
            try:
                services["triage"] = triage_service
                self.logger.info("‚úÖ TriageService inicializado (Llama 4 Scout + Claude + GPT-4o)")
            except Exception as e:
                self.logger.warning(f"‚ùå TriageService falhou: {e}")
                services["triage"] = None
            
            try:
                services["lex9000"] = lex9000_integration_service
                self.logger.info("‚úÖ LEX9000IntegrationService inicializado (Grok 4 via OpenRouter)")
            except Exception as e:
                self.logger.warning(f"‚ùå LEX9000IntegrationService falhou: {e}")
                services["lex9000"] = None
            
            try:
                services["state_manager"] = conversation_state_manager
                self.logger.info("‚úÖ ConversationStateManager inicializado")
            except Exception as e:
                self.logger.warning(f"‚ùå ConversationStateManager falhou: {e}")
                services["state_manager"] = None
            
            try:
                services["redis"] = redis_service
                self.logger.info("‚úÖ RedisService inicializado")
            except Exception as e:
                self.logger.warning(f"‚ùå RedisService falhou: {e}")
                services["redis"] = None
            
            # NOVO: LangChain-Grok para agentes (PRIORIDADE 1 para LEX-9000 e Enhancement)
            try:
                if LANGCHAIN_GROK_AVAILABLE:
                    # Configurar LangChain-Grok para agentes especializados
                    services["langchain_grok"] = ChatXAI(
                        api_key=os.getenv("XAI_API_KEY"),
                        model="grok-4",
                        temperature=0.1,
                        max_tokens=4000
                    )
                    self.logger.info("‚úÖ LangChain-Grok inicializado para agentes (Grok 4)")
                else:
                    services["langchain_grok"] = None
                    self.logger.warning("‚ö†Ô∏è LangChain-Grok n√£o dispon√≠vel")
            except Exception as e:
                self.logger.warning(f"‚ùå LangChain-Grok falhou: {e}")
                services["langchain_grok"] = None
        else:
            self.logger.warning("‚ö†Ô∏è Servi√ßos n√£o dispon√≠veis - usando simula√ß√£o")
            services = {
                "interviewer": None,
                "triage": None,
                "lex9000": None,
                "state_manager": None,
                "redis": None,
                "langchain_grok": None
            }
        
        return services
    
    def _build_langgraph_workflow(self) -> StateGraph:
        """Constr√≥i o workflow declarativo com LangGraph 0.4."""
        
        # Inicializar grafo com estado tipado
        workflow = StateGraph(TriageState)
        
        # ===== ADICIONAR TODOS OS N√ìS =====
        workflow.add_node("start_conversation", self._start_conversation_node)
        workflow.add_node("collect_case_details", self._collect_case_details_node)
        workflow.add_node("detect_complexity", self._detect_complexity_node)
        workflow.add_node("basic_triage", self._basic_triage_node)
        workflow.add_node("lex9000_analysis", self._lex9000_analysis_node)
        workflow.add_node("find_initial_matches", self._find_initial_matches_node)
        workflow.add_node("enhance_matches_with_llm", self._enhance_matches_node)
        workflow.add_node("generate_explanations", self._generate_explanations_node)
        workflow.add_node("send_notifications", self._send_notifications_node)
        workflow.add_node("handle_error", self._handle_error_node)
        
        # ===== DEFINIR ENTRY POINT =====
        workflow.set_entry_point("start_conversation")
        
        # ===== EDGES LINEARES =====
        workflow.add_edge("start_conversation", "collect_case_details")
        workflow.add_edge("collect_case_details", "detect_complexity")
        workflow.add_edge("detect_complexity", "basic_triage")
        
        # ===== EDGE CONDICIONAL: Usar LEX-9000? =====
        workflow.add_conditional_edges(
            "basic_triage",
            RunnableLambda(self._should_use_lex9000),
            {
                "lex9000_analysis": "lex9000_analysis",
                "find_matches": "find_initial_matches",
                "handle_error": "handle_error"
            }
        )
        
        # LEX-9000 sempre vai para matches
        workflow.add_edge("lex9000_analysis", "find_initial_matches")
        
        # ===== EDGE CONDICIONAL: Usar LLM Enhancement? =====
        workflow.add_conditional_edges(
            "find_initial_matches",
            RunnableLambda(self._should_enhance_matches),
            {
                "enhance_matches": "enhance_matches_with_llm",
                "generate_explanations": "generate_explanations",
                "handle_error": "handle_error"
            }
        )
        
        # Enhancement vai para explica√ß√µes
        workflow.add_edge("enhance_matches_with_llm", "generate_explanations")
        
        # Explica√ß√µes sempre v√£o para notifica√ß√µes
        workflow.add_edge("generate_explanations", "send_notifications")
        
        # ===== FINALIZA√á√ïES =====
        workflow.add_edge("send_notifications", END)
        workflow.add_edge("handle_error", END)
        
        return workflow
    
    def _compile_workflow(self) -> Any:
        """Compila o workflow com checkpointing e interrupts."""
        if not LANGGRAPH_AVAILABLE:
            return None
        
        # Configurar checkpoint para persist√™ncia de estado
        memory = MemorySaver()
        
        # Compilar com interrupts espec√≠ficos
        compiled = self.workflow.compile(
            checkpointer=memory,
            interrupt_before=["lex9000_analysis"],  # Pausa antes do LEX-9000
            interrupt_after=["enhance_matches_with_llm"]  # Pausa ap√≥s enhancement
        )
        
        return compiled
    
    # ===== IMPLEMENTA√á√ÉO DOS N√ìS =====
    
    async def _start_conversation_node(self, state: TriageState) -> TriageState:
        """N√≥ que inicia a conversa inteligente."""
        start_time = time.time()
        self.logger.info(f"üöÄ Iniciando conversa para usu√°rio {state['user_id']}")
        
        try:
            # Usar IntelligentInterviewerService real
            if self.services["interviewer"]:
                case_id, first_message = await self.services["interviewer"].start_conversation(state['user_id'])
                
                # Publicar evento no Redis se dispon√≠vel
                if self.services["redis"]:
                    await self.services["redis"].publish(
                        f"triage_events:{case_id}",
                        {
                            "event": "conversation_started",
                            "data": {"case_id": case_id, "first_message": first_message},
                            "timestamp": datetime.now().isoformat()
                        }
                    )
            
            duration = time.time() - start_time
                current_step_durations = state.get("step_durations", [])
                new_step_duration = {"step": "start_conversation", "duration": duration, "status": "success"}
            
            return {
                **state,
                "case_id": case_id,
                    "conversation_data": {"first_message": first_message},
                "current_step": "conversation_started",
                    "step_durations": current_step_durations + [new_step_duration]
                }
            else:
                # Fallback simulado
                await asyncio.sleep(0.1)
                case_id = f"case_{state['user_id']}_{int(time.time())}"
                first_message = "Ol√°! Sou o Justus, seu assistente jur√≠dico. Como posso ajud√°-lo hoje?"
                
                duration = time.time() - start_time
                current_step_durations = state.get("step_durations", [])
                new_step_duration = {"step": "start_conversation", "duration": duration, "status": "success"}
                
                return {
                    **state,
                    "case_id": case_id,
                    "conversation_data": {"first_message": first_message},
                    "current_step": "conversation_started",
                    "step_durations": current_step_durations + [new_step_duration]
            }
            
        except Exception as e:
            duration = time.time() - start_time
            self.logger.error(f"‚ùå Erro ao iniciar conversa: {e}")
            current_step_durations = state.get("step_durations", [])
            new_step_duration = {"step": "start_conversation", "duration": duration, "status": "error"}
            
            return {
                **state,
                "error_context": f"Erro ao iniciar conversa: {str(e)}",
                "current_step": "error",
                "step_durations": current_step_durations + [new_step_duration]
            }
    
    async def _collect_case_details_node(self, state: TriageState) -> TriageState:
        """N√≥ que coleta detalhes do caso."""
        start_time = time.time()
        self.logger.info(f"üìù Coletando detalhes do caso {state['case_id']}")
        
        try:
            # Simular coleta de detalhes (em produ√ß√£o, seria via conversa)
            await asyncio.sleep(0.2)
            
            # Dados simulados do caso (em produ√ß√£o viriam da conversa)
            case_details = {
                "area": "Direito do Trabalho",
                "subarea": "Horas Extras",
                "urgency_h": 48,
                "summary": "Funcion√°rio com horas extras n√£o pagas h√° 6 meses",
                "keywords": ["horas extras", "n√£o pagas", "trabalhista"],
                "sentiment": "Negativo",
                "complexity_factors": ["m√∫ltiplos meses", "valores significativos"]
            }
            
            # Merge manual dos dados
            current_conversation_data = state.get("conversation_data", {})
            updated_conversation_data = {**current_conversation_data, **case_details}
            
            duration = time.time() - start_time
            current_step_durations = state.get("step_durations", [])
            new_step_duration = {"step": "collect_case_details", "duration": duration, "status": "success"}
            
            return {
                **state,
                "conversation_data": updated_conversation_data,
                "current_step": "case_details_collected",
                "step_durations": current_step_durations + [new_step_duration]
            }
            
        except Exception as e:
            duration = time.time() - start_time
            self.logger.error(f"‚ùå Erro ao coletar detalhes: {e}")
            current_step_durations = state.get("step_durations", [])
            new_step_duration = {"step": "collect_case_details", "duration": duration, "status": "error"}
            
            return {
                **state,
                "error_context": f"Erro ao coletar detalhes: {str(e)}",
                "current_step": "error",
                "step_durations": current_step_durations + [new_step_duration]
            }
    
    async def _detect_complexity_node(self, state: TriageState) -> TriageState:
        """N√≥ que detecta complexidade do caso usando l√≥gica espec√≠fica de modelos."""
        start_time = time.time()
        self.logger.info(f"üîç Detectando complexidade do caso {state['case_id']}")
        
        try:
            # CONSERVAR L√ìGICA ESPEC√çFICA: TriageService com modelos especializados
            if self.services["triage"]:
                # Preparar dados para an√°lise
                case_data = state.get("conversation_data", {})
                text_for_analysis = f"""
                √Årea: {case_data.get('area', 'N√£o identificado')}
                Sub√°rea: {case_data.get('subarea', 'Geral')}
                Resumo: {case_data.get('summary', 'Sem resumo')}
                Urg√™ncia: {case_data.get('urgency_h', 72)} horas
                Fatores de complexidade: {', '.join(case_data.get('complexity_factors', []))}
                """
                
                # CONSERVAR ESTRAT√âGIA DE MODELOS POR COMPLEXIDADE:
                # - Casos Simples: Llama 4 Scout (custo m√≠nimo)
                # - Casos M√©dios: Llama 4 Scout + GPT-4o (failover)
                # - Casos Complexos: Ensemble (Claude Sonnet + GPT-4o)
                
                # Detectar complexidade preliminar para escolher estrat√©gia
                preliminary_complexity = self._assess_preliminary_complexity(case_data)
                
                if preliminary_complexity == "simple":
                    # ESTRAT√âGIA SIMPLES: Llama 4 Scout
                    self.logger.info("üü¢ Caso simples detectado - usando Llama 4 Scout")
                    complexity_result = await self.services["triage"]._run_llama_triage(
                        text_for_analysis, 
                        "meta-llama/Llama-4-Scout"
                    )
                elif preliminary_complexity == "medium":
                    # ESTRAT√âGIA M√âDIA: Llama 4 Scout + GPT-4o failover
                    self.logger.info("üü° Caso m√©dio detectado - usando Llama 4 Scout + GPT-4o failover")
                    complexity_result = await self.services["triage"]._run_failover_strategy(text_for_analysis)
            else:
                    # ESTRAT√âGIA COMPLEXA: Ensemble (Claude Sonnet + GPT-4o)
                    self.logger.info("üî¥ Caso complexo detectado - usando Ensemble (Claude Sonnet + GPT-4o)")
                    complexity_result = await self.services["triage"]._run_ensemble_strategy(text_for_analysis)
                
                # Determinar complexidade final baseada no resultado
                complexity_level = self._determine_final_complexity(complexity_result)
                
            else:
                # Fallback simulado
                await asyncio.sleep(0.3)
                complexity_level = "medium"  # Simulado
            
            duration = time.time() - start_time
            current_step_durations = state.get("step_durations", [])
            new_step_duration = {"step": "detect_complexity", "duration": duration, "status": "success"}
            
            return {
                **state,
                "complexity_level": complexity_level,
                "current_step": "complexity_detected",
                "step_durations": current_step_durations + [new_step_duration]
            }
            
        except Exception as e:
            duration = time.time() - start_time
            self.logger.error(f"‚ùå Erro ao detectar complexidade: {e}")
            current_step_durations = state.get("step_durations", [])
            new_step_duration = {"step": "detect_complexity", "duration": duration, "status": "error"}
            
            return {
                **state,
                "complexity_level": "medium",  # Fallback
                "error_context": f"Erro ao detectar complexidade: {str(e)}",
                "current_step": "error",
                "step_durations": current_step_durations + [new_step_duration]
            }
    
    def _assess_preliminary_complexity(self, case_data: Dict[str, Any]) -> str:
        """Avalia complexidade preliminar para escolher estrat√©gia de modelo."""
        # Fatores para complexidade simples
        simple_indicators = [
            "multa", "tr√¢nsito", "consumidor", "produto", "voo", "atraso",
            "cobran√ßa", "indevida", "vizinho", "ru√≠do", "vazamento", "velocidade"
        ]
        
        # Fatores para complexidade alta
        complex_indicators = [
            "empresarial", "societ√°rio", "arbitragem", "fus√£o", "aquisi√ß√£o",
            "patente", "marca", "concorrencial", "regulat√≥rio", "internacional",
            "tribut√°rio", "previdenci√°rio", "ambiental", "eleitoral", "societ√°rias"
        ]
        
        # Fatores para complexidade m√©dia
        medium_indicators = [
            "trabalhista", "trabalho", "horas", "extras", "rescis√£o", "ass√©dio",
            "fam√≠lia", "div√≥rcio", "guarda", "pens√£o", "sucess√£o", "heran√ßa",
            "imobili√°rio", "loca√ß√£o", "condom√≠nio", "constru√ß√£o"
        ]
        
        text = f"{case_data.get('summary', '')} {case_data.get('subarea', '')}".lower()
        
        # Contar indicadores
        simple_count = sum(1 for indicator in simple_indicators if indicator in text)
        complex_count = sum(1 for indicator in complex_indicators if indicator in text)
        medium_count = sum(1 for indicator in medium_indicators if indicator in text)
        
        # Determinar complexidade com prioridade
        if complex_count > 0:
            return "high"
        elif simple_count > 0 and medium_count == 0 and complex_count == 0:
            return "low"
        elif medium_count > 0:
            return "medium"
        else:
            # Fallback baseado no contexto
            if "trabalhista" in text or "fam√≠lia" in text:
                return "medium"
            elif "empresarial" in text or "societ√°rio" in text:
                return "high"
            else:
                return "medium"  # Default para casos n√£o claros
    
    def _determine_final_complexity(self, complexity_result: Dict[str, Any]) -> str:
        """Determina complexidade final baseada no resultado da an√°lise."""
        # Analisar resultado para determinar complexidade
        text = str(complexity_result).lower()
        
        if any(word in text for word in ["complexo", "dif√≠cil", "elaborado", "intricado"]):
            return "high"
        elif any(word in text for word in ["simples", "b√°sico", "rotineiro", "padr√£o"]):
            return "low"
        else:
            return "medium"
    
    async def _basic_triage_node(self, state: TriageState) -> TriageState:
        """N√≥ que executa triagem b√°sica usando l√≥gica espec√≠fica de modelos."""
        start_time = time.time()
        self.logger.info(f"üîß Executando triagem b√°sica para caso {state['case_id']}")
        
        try:
            # CONSERVAR L√ìGICA ESPEC√çFICA: TriageService com modelos especializados
            if self.services["triage"]:
                case_data = state.get("conversation_data", {})
                complexity_level = state.get("complexity_level", "medium")
                
                text_for_triage = f"""
                √Årea: {case_data.get('area', 'N√£o identificado')}
                Sub√°rea: {case_data.get('subarea', 'Geral')}
                Resumo: {case_data.get('summary', 'Sem resumo')}
                Urg√™ncia: {case_data.get('urgency_h', 72)} horas
                Complexidade: {complexity_level}
                """
                
                # CONSERVAR ESTRAT√âGIA DE MODELOS POR COMPLEXIDADE:
                if complexity_level == "low":
                    # CASOS SIMPLES: Llama 4 Scout (custo m√≠nimo)
                    self.logger.info("üü¢ Triagem simples - usando Llama 4 Scout")
                    triage_result = await self.services["triage"]._run_llama_triage(
                        text_for_triage, 
                        "meta-llama/Llama-4-Scout"
                    )
                elif complexity_level == "medium":
                    # CASOS M√âDIOS: Llama 4 Scout + GPT-4o (failover)
                    self.logger.info("üü° Triagem m√©dia - usando Llama 4 Scout + GPT-4o failover")
                    triage_result = await self.services["triage"]._run_failover_strategy(text_for_triage)
                else:
                    # CASOS COMPLEXOS: Ensemble (Claude Sonnet + GPT-4o)
                    self.logger.info("üî¥ Triagem complexa - usando Ensemble (Claude Sonnet + GPT-4o)")
                    triage_result = await self.services["triage"]._run_ensemble_strategy(text_for_triage)
                
                # Adicionar case_type se dispon√≠vel
                if "area" in case_data:
                    try:
                        case_type = map_area_to_case_type(
                            area=case_data.get("area"),
                            subarea=case_data.get("subarea"),
                            keywords=case_data.get("keywords", []),
                            summary=case_data.get("summary"),
                            nature=case_data.get("nature")
                        )
                        triage_result["case_type"] = case_type
                    except Exception as e:
                        self.logger.warning(f"Erro ao mapear case_type: {e}")
                
                # Gerar embedding se dispon√≠vel
                if "summary" in triage_result:
                    try:
                        embedding = await generate_embedding(triage_result["summary"])
                        triage_result["summary_embedding"] = embedding
                    except Exception as e:
                        self.logger.warning(f"Erro ao gerar embedding: {e}")
            else:
                # Fallback simulado
                await asyncio.sleep(0.4)
            triage_result = {
                    "area": "Direito do Trabalho",
                    "subarea": "Horas Extras",
                    "urgency_h": 48,
                    "summary": "Caso de horas extras n√£o pagas",
                    "complexity_level": state.get("complexity_level", "medium"),
                    "model_used": "simulated",
                    "strategy_used": "fallback"
            }
            
            duration = time.time() - start_time
            current_step_durations = state.get("step_durations", [])
            new_step_duration = {"step": "basic_triage", "duration": duration, "status": "success"}
            
            return {
                **state,
                "basic_triage_result": triage_result,
                "current_step": "basic_triage_completed",
                "step_durations": current_step_durations + [new_step_duration]
            }
            
        except Exception as e:
            duration = time.time() - start_time
            self.logger.error(f"‚ùå Erro na triagem b√°sica: {e}")
            current_step_durations = state.get("step_durations", [])
            new_step_duration = {"step": "basic_triage", "duration": duration, "status": "error"}
            
            return {
                **state,
                "error_context": f"Erro na triagem b√°sica: {str(e)}",
                "current_step": "error",
                "step_durations": current_step_durations + [new_step_duration]
            }
    
    async def _lex9000_analysis_node(self, state: TriageState) -> TriageState:
        """N√≥ que executa an√°lise detalhada LEX-9000 usando l√≥gica espec√≠fica de modelos."""
        start_time = time.time()
        self.logger.info(f"ü§ñ Executando an√°lise LEX-9000 para caso {state['case_id']}")
        
        try:
            # CONSERVAR L√ìGICA ESPEC√çFICA: LEX-9000 com modelos especializados
            complexity_level = state.get("complexity_level", "medium")
            
            # CONSERVAR ESTRAT√âGIA: LEX-9000 s√≥ para casos complexos
            if complexity_level == "low":
                self.logger.info("üü¢ Caso simples - LEX-9000 n√£o necess√°rio")
                lex_analysis = {
                    "analysis_type": "lex9000_skipped_simple_case",
                    "reason": "Caso simples n√£o requer an√°lise LEX-9000",
                    "model_used": "none",
                    "sdk_used": "none"
                }
            else:
                # CASOS M√âDIOS E COMPLEXOS: Usar LEX-9000
                case_data = state.get("conversation_data", {})
                
                # Preparar dados para an√°lise LEX-9000
                conversation_data = {
                    "area": case_data.get("area"),
                    "subarea": case_data.get("subarea"),
                    "summary": case_data.get("summary"),
                    "urgency_h": case_data.get("urgency_h"),
                    "keywords": case_data.get("keywords", []),
                    "complexity_factors": case_data.get("complexity_factors", [])
                }
                
                # PRIORIDADE 1: Usar LangChain-Grok se dispon√≠vel (melhor para agentes)
                if self.services.get("langchain_grok"):
                    self.logger.info("üöÄ Usando LangChain-Grok para an√°lise LEX-9000")
                    
                    # Preparar mensagens para LangChain-Grok
                    system_prompt = """
                    Voc√™ √© o "LEX-9000", um assistente jur√≠dico especializado em Direito Brasileiro.
                    Analise o caso e forne√ßa uma an√°lise jur√≠dica estruturada em JSON.
                    
                    Retorne APENAS um JSON v√°lido com:
                    {
                        "classificacao": {
                            "area_principal": "√Årea do direito",
                            "assunto_principal": "Assunto espec√≠fico",
                            "natureza": "Preventivo|Contencioso"
                        },
                        "viabilidade": {
                            "classificacao": "Vi√°vel|Parcialmente Vi√°vel|Invi√°vel",
                            "probabilidade_exito": "Alta|M√©dia|Baixa",
                            "complexidade": "Baixa|M√©dia|Alta"
                        },
                        "aspectos_tecnicos": {
                            "legislacao_aplicavel": ["Lei X, art. Y"],
                            "jurisprudencia_relevante": ["STF/STJ Tema X"],
                            "competencia": "Justi√ßa espec√≠fica"
                        }
                    }
                    """
                    
                    user_message = f"""
                    Analise o seguinte caso jur√≠dico:
                    
                    √Årea: {conversation_data.get('area', 'N/A')}
                    Sub√°rea: {conversation_data.get('subarea', 'N/A')}
                    Resumo: {conversation_data.get('summary', 'N/A')}
                    Urg√™ncia: {conversation_data.get('urgency_h', 72)} horas
                    Fatores de complexidade: {', '.join(conversation_data.get('complexity_factors', []))}
                    """
                    
                    # Executar an√°lise com LangChain-Grok
                    lc_messages = [
                        SystemMessage(content=system_prompt),
                        HumanMessage(content=user_message)
                    ]
                    
                    response = await self.services["langchain_grok"].ainvoke(lc_messages)
                    
                    # Parse da resposta JSON
                    try:
                        import json
                        analysis_data = json.loads(response.content)
                        
                        lex_analysis = {
                            "analysis_type": "detailed_legal_analysis_langchain_grok",
                            "classificacao": analysis_data.get("classificacao", {}),
                            "viabilidade": analysis_data.get("viabilidade", {}),
                            "aspectos_tecnicos": analysis_data.get("aspectos_tecnicos", {}),
                            "confidence": 0.95,  # LangChain-Grok tem alta confian√ßa
                            "model_used": "grok-4-via-langchain",
                            "sdk_used": "langchain_xai"
                        }
                        
                        self.logger.info("‚úÖ An√°lise LEX-9000 conclu√≠da com LangChain-Grok")
                        
                    except json.JSONDecodeError as e:
                        self.logger.warning(f"‚ùå Erro ao parsear JSON do LangChain-Grok: {e}")
                        # Fallback para an√°lise estruturada
                        lex_analysis = {
                            "analysis_type": "detailed_legal_analysis_langchain_grok_fallback",
                            "classificacao": {
                                "area_principal": conversation_data.get("area", "N√£o identificado"),
                                "assunto_principal": conversation_data.get("subarea", "Geral"),
                                "natureza": "Contencioso"
                            },
                            "viabilidade": {
                                "classificacao": "Vi√°vel",
                                "probabilidade_exito": "M√©dia",
                                "complexidade": "M√©dia"
                            },
                            "aspectos_tecnicos": {
                                "legislacao_aplicavel": ["Legisla√ß√£o aplic√°vel"],
                                "jurisprudencia_relevante": ["Jurisprud√™ncia relevante"],
                                "competencia": "Justi√ßa competente"
                            },
                            "confidence": 0.85,
                            "model_used": "grok-4-via-langchain-fallback",
                            "sdk_used": "langchain_xai"
                        }
                
                # PRIORIDADE 2: Usar LEX9000IntegrationService real (OpenRouter)
                elif self.services["lex9000"] and self.services["lex9000"].is_available():
                    self.logger.info("üîÑ Usando LEX9000IntegrationService (Grok 4 via OpenRouter)")
                    
                    # Executar an√°lise LEX-9000 real (Grok 4 via OpenRouter)
                    lex_result = await self.services["lex9000"].analyze_complex_case(conversation_data)
                    
                    # Converter para formato esperado
                    lex_analysis = {
                "analysis_type": "detailed_legal_analysis",
                        "classificacao": lex_result.classificacao,
                        "viabilidade": lex_result.analise_viabilidade,
                        "aspectos_tecnicos": lex_result.aspectos_tecnicos,
                        "confidence": lex_result.confidence_score,
                        "processing_time_ms": lex_result.processing_time_ms,
                        "model_used": "grok-4-via-openrouter",
                        "sdk_used": "openrouter"
                    }
                
                # PRIORIDADE 3: Fallback simulado
                else:
                    self.logger.warning("‚ö†Ô∏è Usando fallback simulado para LEX-9000")
                    # Fallback simulado
                    await asyncio.sleep(1.0)  # LEX-9000 √© mais demorado
                    
                    lex_analysis = {
                        "analysis_type": "detailed_legal_analysis_simulated",
                "classificacao": {
                    "area_principal": "Direito do Trabalho",
                    "assunto_principal": "Horas Extras N√£o Pagas",
                    "natureza": "Contencioso"
                },
                "viabilidade": {
                    "classificacao": "Vi√°vel",
                    "probabilidade_exito": "Alta",
                    "complexidade": "M√©dia"
                },
                "aspectos_tecnicos": {
                    "legislacao_aplicavel": ["CLT Art. 59", "S√∫mula 85 TST"],
                    "jurisprudencia_relevante": ["TST-RR-123456"],
                    "competencia": "Vara do Trabalho"
                },
                        "confidence": 0.92,
                        "model_used": "simulated",
                        "sdk_used": "simulation"
            }
            
            duration = time.time() - start_time
            
            return {
                **state,
                "lex_analysis": lex_analysis,
                "current_step": "lex9000_completed",
                "step_durations": [{"step": "lex9000_analysis", "duration": duration, "status": "success"}]
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro no LEX-9000: {e}")
            current_step_durations = state.get("step_durations", [])
            new_step_duration = {"step": "lex9000_analysis", "duration": time.time() - start_time, "status": "error"}
            
            return {
                **state,
                "error_context": f"Erro no LEX-9000: {str(e)}",
                "current_step": "error",
                "step_durations": current_step_durations + [new_step_duration]
            }
    
    async def _find_initial_matches_node(self, state: TriageState) -> TriageState:
        """N√≥ que encontra matches iniciais de advogados."""
        start_time = time.time()
        self.logger.info(f"üîç Buscando matches iniciais para caso {state['case_id']}")
        
        try:
            # Usar MatchService real se dispon√≠vel
            if self.services.get("matching"):
                # Preparar dados para matching
                triage_data = state.get("basic_triage_result", {})
                lex_analysis = state.get("lex_analysis")
                
                # Criar MatchRequest
                match_request = MatchRequest(
                    area=triage_data.get("area"),
                    subarea=triage_data.get("subarea"),
                    urgency_h=triage_data.get("urgency_h", 72),
                    summary=triage_data.get("summary"),
                    keywords=triage_data.get("keywords", []),
                    complexity_level=state.get("complexity_level", "medium"),
                    lex_analysis=lex_analysis
                )
                
                # Executar matching real
                matches = await find_and_notify_matches(match_request)
            else:
                # Fallback simulado
            await asyncio.sleep(0.3)
            
            matches = [
                {
                    "lawyer_id": "adv_001",
                    "name": "Dr. Ana Santos",
                    "specialization": "Direito Trabalhista",
                    "experience_years": 15,
                    "success_rate": 0.92,
                    "location": "S√£o Paulo",
                    "base_score": 0.87
                },
                {
                    "lawyer_id": "adv_002",
                    "name": "Dr. Carlos Silva",
                    "specialization": "Direito Trabalhista",
                    "experience_years": 12,
                    "success_rate": 0.89,
                    "location": "S√£o Paulo",
                    "base_score": 0.83
                },
                {
                    "lawyer_id": "adv_003",
                    "name": "Dra. Maria Costa",
                    "specialization": "Direito Trabalhista",
                    "experience_years": 18,
                    "success_rate": 0.94,
                    "location": "Rio de Janeiro",
                    "base_score": 0.81
                }
            ]
            
            duration = time.time() - start_time
            current_step_durations = state.get("step_durations", [])
            new_step_duration = {"step": "find_matches", "duration": duration, "status": "success"}
            
            return {
                **state,
                "initial_matches": matches,
                "current_step": "initial_matches_found",
                "step_durations": current_step_durations + [new_step_duration]
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro na busca de matches: {e}")
            current_step_durations = state.get("step_durations", [])
            new_step_duration = {"step": "find_matches", "duration": time.time() - start_time, "status": "error"}
            
            return {
                **state,
                "error_context": f"Erro na busca de matches: {str(e)}",
                "current_step": "error",
                "step_durations": current_step_durations + [new_step_duration]
            }
    
    async def _enhance_matches_node(self, state: TriageState) -> TriageState:
        """N√≥ que melhora matches com LLM."""
        start_time = time.time()
        self.logger.info(f"üöÄ Melhorando matches com LLM para caso {state['case_id']}")
        
        try:
            # PRIORIDADE 1: Usar LangChain-Grok para enhancement
            if self.services.get("langchain_grok"):
                self.logger.info("üöÄ Usando LangChain-Grok para enhancement de matches")
                initial_matches = state.get("initial_matches", [])
                triage_data = state.get("basic_triage_result", {})
                lex_analysis = state.get("lex_analysis")
                
                # Preparar contexto para enhancement
                context = f"""
                Caso: {triage_data.get('summary', 'N/A')}
                √Årea: {triage_data.get('area', 'N/A')}
                Sub√°rea: {triage_data.get('subarea', 'N/A')}
                Complexidade: {state.get('complexity_level', 'medium')}
                
                An√°lise LEX-9000: {lex_analysis.get('classificacao', {}).get('area_principal', 'N/A') if lex_analysis else 'N/A'}
                
                Matches iniciais: {len(initial_matches)} advogados encontrados
                """
                
                # Preparar mensagens para LangChain-Grok
                system_prompt = """
                Voc√™ √© um especialista em matching jur√≠dico. Analise os advogados dispon√≠veis e 
                forne√ßa scores de compatibilidade e raz√µes para recomenda√ß√£o.
                
                Retorne APENAS um JSON v√°lido com:
                {
                    "enhanced_matches": [
                        {
                            "lawyer_id": "ID do advogado",
                            "compatibility_score": 0.85,
                            "enhanced_reasoning": "Raz√£o da recomenda√ß√£o",
                            "recommended_for_case": true
                        }
                    ]
                }
                """
                
                user_message = f"""
                Analise a compatibilidade dos seguintes advogados para o caso:
                
                {context}
                
                Advogados dispon√≠veis:
                {json.dumps(initial_matches, indent=2)}
                """
                
                # Executar enhancement com LangChain-Grok
                lc_messages = [
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=user_message)
                ]
                
                response = await self.services["langchain_grok"].ainvoke(lc_messages)
                
                # Parse da resposta JSON
                try:
                    import json
                    enhancement_data = json.loads(response.content)
                    enhanced_matches = enhancement_data.get("enhanced_matches", [])
                    
                    # Mesclar dados originais com enhancement
                    for i, enhanced in enumerate(enhanced_matches):
                        if i < len(initial_matches):
                            enhanced_matches[i] = {
                                **initial_matches[i],
                                **enhanced
                            }
                    
                    self.logger.info("‚úÖ Enhancement de matches conclu√≠do com LangChain-Grok")
                    
                except json.JSONDecodeError as e:
                    self.logger.warning(f"‚ùå Erro ao parsear JSON do LangChain-Grok: {e}")
                    # Fallback para enhancement estruturado
            enhanced_matches = []
                    for match in initial_matches:
                        llm_score = 0.85 + (match.get("experience_years", 0) * 0.01)
                        enhanced_match = {
                    **match,
                            "llm_compatibility_score": llm_score,
                            "enhanced_reasoning": f"Advogado com {match.get('experience_years')} anos de experi√™ncia em {match.get('specialization')}",
                            "recommended_for_case": True
                        }
                        enhanced_matches.append(enhanced_match)
            
            # PRIORIDADE 2: Usar TriageService real
            elif self.services.get("triage"):
                initial_matches = state.get("initial_matches", [])
                triage_data = state.get("basic_triage_result", {})
                lex_analysis = state.get("lex_analysis")
                
                # Preparar contexto para enhancement
                context = f"""
                Caso: {triage_data.get('summary', 'N/A')}
                √Årea: {triage_data.get('area', 'N/A')}
                Sub√°rea: {triage_data.get('subarea', 'N/A')}
                Complexidade: {state.get('complexity_level', 'medium')}
                
                An√°lise LEX-9000: {lex_analysis.get('classificacao', {}).get('area_principal', 'N/A') if lex_analysis else 'N/A'}
                
                Matches iniciais: {len(initial_matches)} advogados encontrados
                """
                
                # Simular enhancement com LLM
                await asyncio.sleep(0.5)
                
                enhanced_matches = []
                for match in initial_matches:
                    # Adicionar score de compatibilidade LLM
                    llm_score = 0.85 + (match.get("experience_years", 0) * 0.01)
                    enhanced_match = {
                        **match,
                        "llm_compatibility_score": llm_score,
                        "enhanced_reasoning": f"Advogado com {match.get('experience_years')} anos de experi√™ncia em {match.get('specialization')}",
                        "recommended_for_case": True
                    }
                    enhanced_matches.append(enhanced_match)
                
                # Ordenar por score combinado
                enhanced_matches.sort(key=lambda x: x.get("llm_compatibility_score", 0), reverse=True)
            
            # PRIORIDADE 3: Fallback simulado
            else:
                self.logger.warning("‚ö†Ô∏è Usando fallback simulado para enhancement")
                await asyncio.sleep(0.5)
                enhanced_matches = state.get("initial_matches", [])
            
            duration = time.time() - start_time
            current_step_durations = state.get("step_durations", [])
            new_step_duration = {"step": "enhance_matches", "duration": duration, "status": "success"}
            
            return {
                **state,
                "enhanced_matches": enhanced_matches,
                "current_step": "matches_enhanced",
                "step_durations": current_step_durations + [new_step_duration]
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro no enhancement de matches: {e}")
            current_step_durations = state.get("step_durations", [])
            new_step_duration = {"step": "enhance_matches", "duration": time.time() - start_time, "status": "error"}
            
            return {
                **state,
                "error_context": f"Erro no enhancement de matches: {str(e)}",
                "current_step": "error",
                "step_durations": current_step_durations + [new_step_duration]
            }
    
    async def _generate_explanations_node(self, state: TriageState) -> TriageState:
        """N√≥ que gera explica√ß√µes para matches."""
        start_time = time.time()
        self.logger.info(f"üìù Gerando explica√ß√µes para caso {state['case_id']}")
        
        try:
            # PRIORIDADE 1: Usar LangChain-Grok para explica√ß√µes
            if self.services.get("langchain_grok"):
                self.logger.info("üöÄ Usando LangChain-Grok para gera√ß√£o de explica√ß√µes")
                triage_data = state.get("basic_triage_result", {})
                lex_analysis = state.get("lex_analysis")
                matches = state.get("enhanced_matches", state.get("initial_matches", []))
                
                # Preparar contexto para explica√ß√µes
                context = f"""
                Caso: {triage_data.get('summary', 'N/A')}
                √Årea: {triage_data.get('area', 'N/A')}
                Sub√°rea: {triage_data.get('subarea', 'N/A')}
                Urg√™ncia: {triage_data.get('urgency_h', 72)} horas
                Complexidade: {state.get('complexity_level', 'medium')}
                
                An√°lise LEX-9000: {lex_analysis.get('classificacao', {}).get('area_principal', 'N/A') if lex_analysis else 'N/A'}
                
                Matches encontrados: {len(matches)} advogados
                """
                
                # Preparar mensagens para LangChain-Grok
                system_prompt = """
                Voc√™ √© um especialista jur√≠dico que gera explica√ß√µes claras e detalhadas sobre casos.
                Forne√ßa explica√ß√µes estruturadas em portugu√™s brasileiro.
                
                Retorne APENAS um JSON v√°lido com:
                {
                    "explanations": [
                        {
                            "type": "case_analysis",
                            "content": "An√°lise detalhada do caso",
                            "key_points": ["Ponto 1", "Ponto 2"]
                        },
                        {
                            "type": "legal_advice",
                            "content": "Conselhos jur√≠dicos",
                            "next_steps": ["Passo 1", "Passo 2"]
                        },
                        {
                            "type": "match_recommendation",
                            "content": "Recomenda√ß√µes de advogados",
                            "reasoning": "Raz√µes para as recomenda√ß√µes"
                        }
                    ]
                }
                """
                
                user_message = f"""
                Gere explica√ß√µes detalhadas para o seguinte caso jur√≠dico:
                
                {context}
                
                Dados do caso:
                {json.dumps(triage_data, indent=2)}
                
                An√°lise LEX-9000:
                {json.dumps(lex_analysis, indent=2) if lex_analysis else 'N/A'}
                
                Advogados recomendados:
                {json.dumps(matches[:3], indent=2)}  # Top 3 matches
                """
                
                # Executar gera√ß√£o de explica√ß√µes com LangChain-Grok
                lc_messages = [
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=user_message)
                ]
                
                response = await self.services["langchain_grok"].ainvoke(lc_messages)
                
                # Parse da resposta JSON
                try:
                    import json
                    explanations_data = json.loads(response.content)
            explanations = []
                    
                    for exp in explanations_data.get("explanations", []):
                        explanations.append(exp.get("content", ""))
                    
                    self.logger.info("‚úÖ Explica√ß√µes geradas com LangChain-Grok")
                    
                except json.JSONDecodeError as e:
                    self.logger.warning(f"‚ùå Erro ao parsear JSON do LangChain-Grok: {e}")
                    # Fallback para explica√ß√µes estruturadas
                    explanations = self._generate_fallback_explanations(triage_data, lex_analysis, matches)
            
            # PRIORIDADE 2: Gerar explica√ß√µes estruturadas
            else:
                triage_data = state.get("basic_triage_result", {})
                lex_analysis = state.get("lex_analysis")
                matches = state.get("enhanced_matches", state.get("initial_matches", []))
                
                explanations = self._generate_fallback_explanations(triage_data, lex_analysis, matches)
            
            duration = time.time() - start_time
            current_step_durations = state.get("step_durations", [])
            new_step_duration = {"step": "generate_explanations", "duration": duration, "status": "success"}
            
            return {
                **state,
                "match_explanations": explanations,
                "current_step": "explanations_generated",
                "step_durations": current_step_durations + [new_step_duration]
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro na gera√ß√£o de explica√ß√µes: {e}")
            current_step_durations = state.get("step_durations", [])
            new_step_duration = {"step": "generate_explanations", "duration": time.time() - start_time, "status": "error"}
            
            return {
                **state,
                "error_context": f"Erro na gera√ß√£o de explica√ß√µes: {str(e)}",
                "current_step": "error",
                "step_durations": current_step_durations + [new_step_duration]
            }
    
    def _generate_fallback_explanations(self, triage_data: Dict, lex_analysis: Optional[Dict], matches: List[Dict]) -> List[str]:
        """Gera explica√ß√µes de fallback quando LangChain-Grok n√£o est√° dispon√≠vel."""
        explanations = []
        
        # Explica√ß√£o do caso
        case_explanation = f"""
        **An√°lise do Caso:**
        - √Årea: {triage_data.get('area', 'N/A')}
        - Sub√°rea: {triage_data.get('subarea', 'N/A')}
        - Urg√™ncia: {triage_data.get('urgency_h', 72)} horas
        - Complexidade: {triage_data.get('complexity_level', 'medium')}
        """
        
        if lex_analysis:
            case_explanation += f"""
            **An√°lise LEX-9000:**
            - Viabilidade: {lex_analysis.get('viabilidade', {}).get('classificacao', 'N/A')}
            - Probabilidade de sucesso: {lex_analysis.get('viabilidade', {}).get('probabilidade_exito', 'N/A')}
            - Compet√™ncia: {lex_analysis.get('aspectos_tecnicos', {}).get('competencia', 'N/A')}
            """
        
        explanations.append(case_explanation)
        
        # Explica√ß√£o dos matches
        if matches:
            match_explanation = f"""
            **Recomenda√ß√µes de Advogados:**
            Encontramos {len(matches)} advogados especializados para seu caso.
            
            Principais recomenda√ß√µes:
            """
            
            for i, match in enumerate(matches[:3], 1):
                match_explanation += f"""
                {i}. {match.get('name', 'Advogado')} - {match.get('specialization', 'Especializa√ß√£o')}
                   Experi√™ncia: {match.get('experience_years', 0)} anos
                   Score de compatibilidade: {match.get('llm_compatibility_score', 0.85):.2f}
                """
            
            explanations.append(match_explanation)
        
        return explanations
    
    async def _send_notifications_node(self, state: TriageState) -> TriageState:
        """N√≥ que envia notifica√ß√µes."""
        start_time = time.time()
        self.logger.info(f"üìß Enviando notifica√ß√µes para caso {state['case_id']}")
        
        try:
            # Usar NotifyService real se dispon√≠vel
            if self.services.get("notification"):
                user_id = state.get("user_id")
                case_id = state.get("case_id")
                triage_data = state.get("basic_triage_result", {})
                matches = state.get("enhanced_matches", state.get("initial_matches", []))
                
                # Enviar notifica√ß√£o para o cliente
            notification_data = {
                    "case_id": case_id,
                    "area": triage_data.get("area"),
                    "subarea": triage_data.get("subarea"),
                    "summary": triage_data.get("summary"),
                    "matches_count": len(matches),
                    "complexity_level": state.get("complexity_level")
                }
                
                await send_notification_to_client(user_id, notification_data)
            else:
                # Simular envio de notifica√ß√£o
                await asyncio.sleep(0.2)
                self.logger.info(f"üìß Notifica√ß√£o simulada enviada para usu√°rio {state.get('user_id')}")
            
            duration = time.time() - start_time
            current_step_durations = state.get("step_durations", [])
            new_step_duration = {"step": "send_notifications", "duration": duration, "status": "success"}
            
            return {
                **state,
                "notifications_sent": True,
                "current_step": "notifications_sent",
                "step_durations": current_step_durations + [new_step_duration]
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao enviar notifica√ß√µes: {e}")
            current_step_durations = state.get("step_durations", [])
            new_step_duration = {"step": "send_notifications", "duration": time.time() - start_time, "status": "error"}
            
            return {
                **state,
                "error_context": f"Erro ao enviar notifica√ß√µes: {str(e)}",
                "current_step": "error",
                "step_durations": current_step_durations + [new_step_duration]
            }
    
    async def _handle_error_node(self, state: TriageState) -> TriageState:
        """N√≥ que trata erros do workflow."""
        start_time = time.time()
        self.logger.error(f"üö® Tratando erro no workflow: {state.get('error_context')}")
        
        try:
            # Log do erro
            error_context = state.get("error_context", "Erro desconhecido")
            self.logger.error(f"Workflow error: {error_context}")
            
            # Publicar erro no Redis se dispon√≠vel
            if self.services.get("redis") and state.get("case_id"):
                await self.services["redis"].publish(
                    f"triage_events:{state['case_id']}",
                    {
                        "event": "workflow_error",
                        "data": {"error": error_context},
                        "timestamp": datetime.now().isoformat()
                    }
                )
            
            duration = time.time() - start_time
            current_step_durations = state.get("step_durations", [])
            new_step_duration = {"step": "handle_error", "duration": duration, "status": "error"}
            
            return {
                **state,
                "current_step": "error_handled",
                "step_durations": current_step_durations + [new_step_duration]
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao tratar erro: {e}")
            current_step_durations = state.get("step_durations", [])
            new_step_duration = {"step": "handle_error", "duration": time.time() - start_time, "status": "error"}
            
            return {
                **state,
                "error_context": f"Erro ao tratar erro: {str(e)}",
                "current_step": "error",
                "step_durations": current_step_durations + [new_step_duration]
            }
    
    # ===== FUN√á√ïES DE DECIS√ÉO =====
    
    def _should_use_lex9000(self, state: TriageState) -> str:
        """
        Decide se deve usar an√°lise LEX-9000 baseado na complexidade.
        
        Returns:
            "lex9000_analysis" | "find_matches" | "handle_error"
        """
        try:
            complexity = state.get("complexity_level", "medium")
            triage_data = state.get("basic_triage_result", {})
            
            # Crit√©rios para usar LEX-9000
            use_lex9000 = (
                complexity == "high" or
                complexity == "complex" or
                triage_data.get("area") in ["Empresarial", "Trabalhista", "Criminal"] or
                state.get("lex_analysis") is None  # Ainda n√£o foi executado
            )
            
            if use_lex9000 and self.services.get("lex9000"):
            return "lex9000_analysis"
            elif state.get("error_context"):
                return "handle_error"
        else:
            return "find_matches"
    
        except Exception as e:
            self.logger.error(f"Erro na decis√£o LEX-9000: {e}")
            return "handle_error"
        
    def _should_enhance_matches(self, state: TriageState) -> str:
        """
        Decide se deve melhorar matches com LLM.
        
        Returns:
            "enhance_matches" | "generate_explanations" | "handle_error"
        """
        try:
        matches = state.get("initial_matches", [])
            complexity = state.get("complexity_level", "medium")
            
            # Crit√©rios para enhancement
            should_enhance = (
                len(matches) > 0 and
                complexity in ["medium", "high", "complex"] and
                self.services.get("triage") is not None
            )
            
            if should_enhance:
            return "enhance_matches"
            elif state.get("error_context"):
                return "handle_error"
        else:
            return "generate_explanations"
    
        except Exception as e:
            self.logger.error(f"Erro na decis√£o de enhancement: {e}")
            return "handle_error"
    
    # ===== M√âTODO PRINCIPAL MELHORADO =====
    
    async def start_intelligent_triage(
        self, 
        user_id: str,
        user_preferences: Optional[Dict[str, Any]] = None
    ) -> TriageResult:
        """
        Ponto de entrada p√∫blico - executa o workflow completo.
        
        Args:
            user_id: ID do usu√°rio
            user_preferences: Prefer√™ncias opcionais do usu√°rio
        
        Returns:
            TriageResult com resultado completo
        """
        
        start_time = time.time()
        thread_id = f"triage_{user_id}_{int(start_time)}"
        
        # Estado inicial
        initial_state = TriageState(
            user_id=user_id,
            case_id=None,
            thread_id=thread_id,
            conversation_data={},
            user_preferences=user_preferences or {},
            complexity_level="",
            basic_triage_result={},
            lex_analysis=None,
            initial_matches=[],
            enhanced_matches=[],
            match_explanations=[],
            current_step="starting",
            error_context=None,
            notifications_sent=False,
            workflow_paused=False,
            processing_start_time=start_time,
            step_durations=[]
        )
        
        try:
            # Verificar disponibilidade dos servi√ßos
            service_status = self.get_service_status()
            self.logger.info(f"Status dos servi√ßos: {service_status}")
            
            if LANGGRAPH_AVAILABLE and self.compiled_workflow:
                # Executar via LangGraph 0.4
                self.logger.info("üöÄ Executando workflow via LangGraph 0.4")
                config = {"configurable": {"thread_id": thread_id}}
                final_state = await self.compiled_workflow.ainvoke(initial_state, config)
                
                # Gerar visualiza√ß√£o do workflow
                visualization = self._generate_workflow_visualization()
            else:
                # Fallback para execu√ß√£o simulada
                self.logger.warning("‚ö†Ô∏è Usando execu√ß√£o simulada (LangGraph n√£o dispon√≠vel)")
                final_state = await self._simulate_workflow_execution(initial_state)
                visualization = "Visualiza√ß√£o n√£o dispon√≠vel (LangGraph n√£o instalado)"
            
            # Construir resultado final
            result = TriageResult(
                success=final_state.get("error_context") is None,
                case_id=final_state.get("case_id"),
                triage_result=final_state.get("basic_triage_result", {}),
                lex_analysis=final_state.get("lex_analysis"),
                matches=final_state.get("enhanced_matches", final_state.get("initial_matches", [])),
                explanations=final_state.get("match_explanations", []),
                error=final_state.get("error_context"),
                processing_summary={
                    "total_duration": sum(step["duration"] for step in final_state.get("step_durations", [])),
                    "steps_completed": [step["step"] for step in final_state.get("step_durations", [])],
                    "complexity_level": final_state.get("complexity_level", "unknown"),
                    "lex_analysis_used": final_state.get("lex_analysis") is not None,
                    "llm_enhancement_used": "enhanced_matches" in final_state,
                    "services_available": service_status
                },
                workflow_visualization=visualization
            )
            
            total_time = time.time() - start_time
            self.logger.info(
                f"‚úÖ Triagem inteligente conclu√≠da em {total_time:.2f}s "
                f"para usu√°rio {user_id} (caso: {result.case_id})"
            )
            
            return result
            
        except Exception as e:
            total_time = time.time() - start_time
            self.logger.error(f"‚ùå Erro na triagem inteligente ({total_time:.2f}s): {e}")
            
            return TriageResult(
                success=False,
                case_id=None,
                triage_result={},
                lex_analysis=None,
                matches=[],
                explanations=[],
                error=str(e),
                processing_summary={
                    "total_duration": total_time,
                    "steps_completed": [],
                    "complexity_level": "error",
                    "lex_analysis_used": False,
                    "llm_enhancement_used": False,
                    "services_available": self.get_service_status()
                },
                workflow_visualization=None
            )
    
    async def _simulate_workflow_execution(self, state: TriageState) -> TriageState:
        """Execu√ß√£o simulada do workflow quando LangGraph n√£o est√° dispon√≠vel."""
        self.logger.info("üîÑ Executando workflow simulado")
        
        try:
            # Executar n√≥s em sequ√™ncia
            state = await self._start_conversation_node(state)
            if state.get("error_context"):
                return state
            
            state = await self._collect_case_details_node(state)
            if state.get("error_context"):
                return state
            
            state = await self._detect_complexity_node(state)
            if state.get("error_context"):
                return state
            
            state = await self._basic_triage_node(state)
            if state.get("error_context"):
                return state
            
            # Decis√£o condicional: usar LEX-9000?
            decision = self._should_use_lex9000(state)
            if decision == "lex9000_analysis":
                state = await self._lex9000_analysis_node(state)
                if state.get("error_context"):
                    return state
            
            state = await self._find_initial_matches_node(state)
            if state.get("error_context"):
                return state
            
            # Decis√£o condicional: melhorar matches?
            decision = self._should_enhance_matches(state)
            if decision == "enhance_matches":
                state = await self._enhance_matches_node(state)
                if state.get("error_context"):
                    return state
            
            state = await self._generate_explanations_node(state)
            if state.get("error_context"):
                return state
            
            state = await self._send_notifications_node(state)
            return state
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro na execu√ß√£o simulada: {e}")
            return {
                **state,
                "error_context": f"Erro na execu√ß√£o simulada: {str(e)}",
                "current_step": "error"
            }
    
    def _generate_workflow_visualization(self) -> str:
        """Gera visualiza√ß√£o do workflow LangGraph."""
        if not LANGGRAPH_AVAILABLE:
            return "Visualiza√ß√£o n√£o dispon√≠vel (LangGraph n√£o instalado)"
        
        try:
            # Gerar visualiza√ß√£o b√°sica
            visualization = """
            ```mermaid
graph TD
                A[Start Conversation] --> B[Collect Case Details]
                B --> C[Detect Complexity]
                C --> D[Basic Triage]
                D --> E{Use LEX-9000?}
                E -->|Yes| F[LEX-9000 Analysis]
                E -->|No| G[Find Initial Matches]
    F --> G
                G --> H{Enhance Matches?}
                H -->|Yes| I[Enhance Matches with LLM]
                H -->|No| J[Generate Explanations]
    I --> J
                J --> K[Send Notifications]
                K --> L[End]
                
                E -->|Error| M[Handle Error]
                H -->|Error| M
    M --> L
            ```
"""
            return visualization
        except Exception as e:
            return f"Erro ao gerar visualiza√ß√£o: {e}"
    
    def get_service_status(self) -> Dict[str, Any]:
        """Retorna status dos servi√ßos integrados."""
        return {
            "langgraph_available": LANGGRAPH_AVAILABLE,
            "services_available": SERVICES_AVAILABLE,
            "langchain_grok_available": LANGCHAIN_GROK_AVAILABLE,
            "interviewer_service": self.services.get("interviewer") is not None,
            "triage_service": self.services.get("triage") is not None,
            "lex9000_service": self.services.get("lex9000") is not None,
            "state_manager": self.services.get("state_manager") is not None,
            "redis_service": self.services.get("redis") is not None,
            "langchain_grok_service": self.services.get("langchain_grok") is not None,
            "workflow_compiled": self.compiled_workflow is not None
        }


# ===== FACTORY E TESTE =====

def get_intelligent_triage_orchestrator_v2() -> IntelligentTriageOrchestratorV2:
    """Factory function para obter inst√¢ncia do orquestrador V2."""
    return IntelligentTriageOrchestratorV2()


# Inst√¢ncia singleton
orchestrator_v2 = IntelligentTriageOrchestratorV2()


async def test_v2_orchestrator():
    """Teste completo do orquestrador V2."""
    print("üß™ TESTE DO ORQUESTRADOR V2")
        print("=" * 50)
        
    # Testar inicializa√ß√£o
    orchestrator = get_intelligent_triage_orchestrator_v2()
        status = orchestrator.get_service_status()
    
    print(f"‚úÖ Status dos servi√ßos:")
    for service, available in status.items():
        print(f"   {service}: {'‚úÖ' if available else '‚ùå'}")
    
    # Testar workflow
    print(f"\nüöÄ Testando workflow...")
    result = await orchestrator.start_intelligent_triage("test_user_123")
    
    print(f"\nüìä RESULTADO DO TESTE:")
    print(f"   Sucesso: {'‚úÖ' if result.success else '‚ùå'}")
    print(f"   Case ID: {result.case_id}")
    print(f"   √Årea: {result.triage_result.get('area', 'N/A')}")
    print(f"   Matches: {len(result.matches)} encontrados")
    print(f"   LEX-9000 usado: {'‚úÖ' if result.lex_analysis else '‚ùå'}")
    print(f"   LLM Enhancement: {'‚úÖ' if result.processing_summary.get('llm_enhancement_used') else '‚ùå'}")
    print(f"   Dura√ß√£o total: {result.processing_summary.get('total_duration', 0):.2f}s")
    
    if result.error:
        print(f"   ‚ùå Erro: {result.error}")
    
    print(f"\nüìù Explica√ß√µes geradas: {len(result.explanations)}")
    for i, explanation in enumerate(result.explanations[:2], 1):
        print(f"   {i}. {explanation[:100]}...")
    
    print(f"\nüéØ VISUALIZA√á√ÉO DO WORKFLOW:")
            if result.workflow_visualization:
        print(result.workflow_visualization)
    else:
        print("   Visualiza√ß√£o n√£o dispon√≠vel")
            
    return result
    

if __name__ == "__main__":
    import asyncio
    asyncio.run(test_v2_orchestrator()) 
 