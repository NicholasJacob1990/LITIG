#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Test Script: Cluster Quality Metrics System
===========================================

Script para testar o sistema completo de m√©tricas de qualidade dos clusters.
Verifica todas as funcionalidades implementadas no SPRINT 2.2.

Testes realizados:
1. Servi√ßo de m√©tricas de qualidade
2. Integra√ß√£o com ClusterService  
3. APIs REST de qualidade
4. Valida√ß√£o de thresholds
5. An√°lise de tend√™ncias
6. Dashboard de qualidade

Usage:
    python test_cluster_quality_system.py [--test-type all|unit|integration|api]
"""

import asyncio
import logging
import sys
import json
import argparse
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Imports do sistema
try:
    from database import get_async_session
    from services.cluster_service import ClusterService
    from services.cluster_quality_metrics_service import (
        ClusterQualityMetricsService,
        create_quality_metrics_service,
        QualityThreshold
    )
    from jobs.cluster_generation_job import run_cluster_generation
    IMPORTS_AVAILABLE = True
except ImportError as e:
    logger.error(f"‚ùå Erro ao importar m√≥dulos: {e}")
    IMPORTS_AVAILABLE = False

# Test data e configura√ß√µes
TEST_CONFIG = {
    "sample_cluster_ids": [
        "case_cluster_1",
        "case_cluster_2", 
        "lawyer_cluster_1",
        "lawyer_cluster_2"
    ],
    "custom_thresholds": {
        "silhouette_score": {"fair": 0.4},
        "cohesion_score": {"fair": 0.5},
        "overall_quality": 0.6
    },
    "batch_size": 5,
    "trends_days": 7
}


class ClusterQualitySystemTester:
    """Testador abrangente do sistema de m√©tricas de qualidade."""
    
    def __init__(self):
        self.results = {
            "unit_tests": {},
            "integration_tests": {},
            "api_tests": {},
            "performance_tests": {},
            "overall_status": "pending"
        }
        self.test_start_time = datetime.now()
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """Executa todos os testes do sistema."""
        
        logger.info("üöÄ Iniciando testes completos do sistema de m√©tricas de qualidade")
        
        if not IMPORTS_AVAILABLE:
            self.results["overall_status"] = "failed"
            self.results["error"] = "Imports n√£o dispon√≠veis"
            return self.results
        
        try:
            # 1. Testes unit√°rios
            logger.info("üî¨ Executando testes unit√°rios...")
            await self._run_unit_tests()
            
            # 2. Testes de integra√ß√£o
            logger.info("üîó Executando testes de integra√ß√£o...")
            await self._run_integration_tests()
            
            # 3. Testes de API
            logger.info("üåê Executando testes de API...")
            await self._run_api_tests()
            
            # 4. Testes de performance
            logger.info("‚ö° Executando testes de performance...")
            await self._run_performance_tests()
            
            # 5. Gerar relat√≥rio final
            self._generate_final_report()
            
        except Exception as e:
            logger.error(f"‚ùå Erro geral nos testes: {e}")
            self.results["overall_status"] = "failed"
            self.results["error"] = str(e)
        
        return self.results
    
    async def _run_unit_tests(self):
        """Testes unit√°rios dos componentes."""
        
        unit_results = {}
        
        try:
            async with get_async_session() as db:
                # Test 1: Cria√ß√£o do servi√ßo de m√©tricas
                logger.info("üß™ Teste 1: Cria√ß√£o do servi√ßo de m√©tricas")
                try:
                    quality_service = create_quality_metrics_service(db)
                    unit_results["service_creation"] = {
                        "status": "passed",
                        "service_available": quality_service is not None
                    }
                    logger.info("‚úÖ Servi√ßo de m√©tricas criado com sucesso")
                except Exception as e:
                    unit_results["service_creation"] = {
                        "status": "failed",
                        "error": str(e)
                    }
                    logger.error(f"‚ùå Falha na cria√ß√£o do servi√ßo: {e}")
                
                # Test 2: Configura√ß√£o de thresholds
                logger.info("üß™ Teste 2: Configura√ß√£o de thresholds")
                try:
                    if quality_service:
                        thresholds = quality_service.quality_thresholds
                        expected_keys = ['silhouette_score', 'cohesion_score', 'separation_score']
                        
                        unit_results["threshold_config"] = {
                            "status": "passed" if all(key in thresholds for key in expected_keys) else "failed",
                            "configured_thresholds": list(thresholds.keys()),
                            "expected_thresholds": expected_keys
                        }
                        logger.info("‚úÖ Thresholds configurados corretamente")
                    else:
                        unit_results["threshold_config"] = {
                            "status": "skipped",
                            "reason": "Servi√ßo n√£o dispon√≠vel"
                        }
                except Exception as e:
                    unit_results["threshold_config"] = {
                        "status": "failed",
                        "error": str(e)
                    }
                
                # Test 3: Enum de qualidade
                logger.info("üß™ Teste 3: Enum de n√≠veis de qualidade")
                try:
                    quality_levels = [threshold.name for threshold in QualityThreshold]
                    expected_levels = ['EXCELLENT', 'GOOD', 'FAIR', 'POOR']
                    
                    unit_results["quality_enum"] = {
                        "status": "passed" if set(quality_levels) == set(expected_levels) else "failed",
                        "available_levels": quality_levels,
                        "expected_levels": expected_levels
                    }
                    logger.info("‚úÖ Enum de qualidade funcionando")
                except Exception as e:
                    unit_results["quality_enum"] = {
                        "status": "failed",
                        "error": str(e)
                    }
        
        except Exception as e:
            logger.error(f"‚ùå Erro geral nos testes unit√°rios: {e}")
            unit_results["general_error"] = str(e)
        
        self.results["unit_tests"] = unit_results
    
    async def _run_integration_tests(self):
        """Testes de integra√ß√£o entre componentes."""
        
        integration_results = {}
        
        try:
            async with get_async_session() as db:
                cluster_service = ClusterService(db)
                
                # Test 1: Integra√ß√£o ClusterService + QualityMetrics
                logger.info("üîó Teste 1: Integra√ß√£o ClusterService + QualityMetrics")
                try:
                    has_quality_service = cluster_service.quality_metrics_service is not None
                    integration_results["cluster_quality_integration"] = {
                        "status": "passed" if has_quality_service else "failed",
                        "quality_service_injected": has_quality_service
                    }
                    
                    if has_quality_service:
                        logger.info("‚úÖ Integra√ß√£o ClusterService + QualityMetrics OK")
                    else:
                        logger.warning("‚ö†Ô∏è Servi√ßo de qualidade n√£o injetado no ClusterService")
                        
                except Exception as e:
                    integration_results["cluster_quality_integration"] = {
                        "status": "failed",
                        "error": str(e)
                    }
                
                # Test 2: M√©todos de qualidade no ClusterService
                logger.info("üîó Teste 2: M√©todos de qualidade no ClusterService")
                try:
                    quality_methods = [
                        "analyze_cluster_quality",
                        "validate_cluster_quality_thresholds", 
                        "get_quality_trends_report",
                        "analyze_all_clusters_quality"
                    ]
                    
                    available_methods = []
                    for method in quality_methods:
                        if hasattr(cluster_service, method):
                            available_methods.append(method)
                    
                    integration_results["quality_methods"] = {
                        "status": "passed" if len(available_methods) == len(quality_methods) else "partial",
                        "available_methods": available_methods,
                        "expected_methods": quality_methods,
                        "coverage": len(available_methods) / len(quality_methods)
                    }
                    logger.info(f"‚úÖ M√©todos de qualidade: {len(available_methods)}/{len(quality_methods)}")
                    
                except Exception as e:
                    integration_results["quality_methods"] = {
                        "status": "failed", 
                        "error": str(e)
                    }
                
                # Test 3: Buscar clusters para teste
                logger.info("üîó Teste 3: Buscar clusters para an√°lise")
                try:
                    # Buscar clusters existentes para testar
                    trending_clusters = await cluster_service.get_trending_clusters(
                        cluster_type="case", limit=3
                    )
                    
                    integration_results["cluster_discovery"] = {
                        "status": "passed" if trending_clusters else "warning",
                        "clusters_found": len(trending_clusters),
                        "sample_cluster_ids": [c.get("cluster_id") for c in trending_clusters[:2]]
                    }
                    
                    if trending_clusters:
                        logger.info(f"‚úÖ Encontrados {len(trending_clusters)} clusters para teste")
                        # Atualizar IDs de teste com clusters reais
                        TEST_CONFIG["sample_cluster_ids"] = [
                            c.get("cluster_id") for c in trending_clusters[:2]
                        ]
                    else:
                        logger.warning("‚ö†Ô∏è Nenhum cluster encontrado para teste")
                    
                except Exception as e:
                    integration_results["cluster_discovery"] = {
                        "status": "failed",
                        "error": str(e)
                    }
        
        except Exception as e:
            logger.error(f"‚ùå Erro geral nos testes de integra√ß√£o: {e}")
            integration_results["general_error"] = str(e)
        
        self.results["integration_tests"] = integration_results
    
    async def _run_api_tests(self):
        """Testes das APIs REST de qualidade."""
        
        api_results = {}
        
        try:
            async with get_async_session() as db:
                cluster_service = ClusterService(db)
                
                # Test 1: An√°lise de qualidade de cluster espec√≠fico
                logger.info("üåê Teste 1: API - An√°lise de qualidade espec√≠fica")
                try:
                    sample_clusters = TEST_CONFIG["sample_cluster_ids"]
                    if sample_clusters:
                        test_cluster_id = sample_clusters[0]
                        quality_analysis = await cluster_service.analyze_cluster_quality(
                            test_cluster_id, include_detailed_analysis=False
                        )
                        
                        api_results["specific_analysis"] = {
                            "status": "passed" if quality_analysis else "failed",
                            "cluster_tested": test_cluster_id,
                            "analysis_returned": quality_analysis is not None,
                            "response_structure": list(quality_analysis.keys()) if quality_analysis else []
                        }
                        
                        if quality_analysis:
                            logger.info(f"‚úÖ An√°lise de qualidade para {test_cluster_id}: Score={quality_analysis.get('overall_quality_score', 'N/A')}")
                        else:
                            logger.warning(f"‚ö†Ô∏è Nenhuma an√°lise retornada para {test_cluster_id}")
                    else:
                        api_results["specific_analysis"] = {
                            "status": "skipped",
                            "reason": "Nenhum cluster dispon√≠vel para teste"
                        }
                        
                except Exception as e:
                    api_results["specific_analysis"] = {
                        "status": "failed",
                        "error": str(e)
                    }
                
                # Test 2: Valida√ß√£o de thresholds
                logger.info("üåê Teste 2: API - Valida√ß√£o de thresholds")
                try:
                    sample_clusters = TEST_CONFIG["sample_cluster_ids"]
                    if sample_clusters:
                        test_cluster_id = sample_clusters[0]
                        validation_result = await cluster_service.validate_cluster_quality_thresholds(
                            test_cluster_id, TEST_CONFIG["custom_thresholds"]
                        )
                        
                        api_results["threshold_validation"] = {
                            "status": "passed" if validation_result else "failed",
                            "cluster_tested": test_cluster_id,
                            "validation_returned": validation_result is not None,
                            "is_valid": validation_result.get("valid") if validation_result else None
                        }
                        
                        if validation_result:
                            logger.info(f"‚úÖ Valida√ß√£o para {test_cluster_id}: V√°lido={validation_result.get('valid', 'N/A')}")
                        else:
                            logger.warning(f"‚ö†Ô∏è Nenhuma valida√ß√£o retornada para {test_cluster_id}")
                    else:
                        api_results["threshold_validation"] = {
                            "status": "skipped",
                            "reason": "Nenhum cluster dispon√≠vel para teste"
                        }
                        
                except Exception as e:
                    api_results["threshold_validation"] = {
                        "status": "failed",
                        "error": str(e)
                    }
                
                # Test 3: Relat√≥rio de tend√™ncias
                logger.info("üåê Teste 3: API - Relat√≥rio de tend√™ncias")
                try:
                    trends_report = await cluster_service.get_quality_trends_report(
                        TEST_CONFIG["trends_days"]
                    )
                    
                    api_results["trends_report"] = {
                        "status": "passed" if trends_report else "failed",
                        "days_analyzed": TEST_CONFIG["trends_days"],
                        "report_returned": trends_report is not None,
                        "report_sections": list(trends_report.keys()) if trends_report else []
                    }
                    
                    if trends_report:
                        logger.info(f"‚úÖ Relat√≥rio de tend√™ncias gerado: {len(trends_report)} se√ß√µes")
                    else:
                        logger.warning("‚ö†Ô∏è Nenhum relat√≥rio de tend√™ncias retornado")
                        
                except Exception as e:
                    api_results["trends_report"] = {
                        "status": "failed",
                        "error": str(e)
                    }
                
                # Test 4: An√°lise em lote
                logger.info("üåê Teste 4: API - An√°lise em lote")
                try:
                    batch_analysis = await cluster_service.analyze_all_clusters_quality(
                        cluster_type="case", batch_size=TEST_CONFIG["batch_size"]
                    )
                    
                    api_results["batch_analysis"] = {
                        "status": "passed" if batch_analysis else "failed",
                        "batch_size": TEST_CONFIG["batch_size"],
                        "analysis_returned": batch_analysis is not None,
                        "clusters_analyzed": batch_analysis.get("total_analyzed", 0) if batch_analysis else 0
                    }
                    
                    if batch_analysis:
                        analyzed_count = batch_analysis.get("total_analyzed", 0)
                        logger.info(f"‚úÖ An√°lise em lote: {analyzed_count} clusters analisados")
                    else:
                        logger.warning("‚ö†Ô∏è Nenhuma an√°lise em lote retornada")
                        
                except Exception as e:
                    api_results["batch_analysis"] = {
                        "status": "failed",
                        "error": str(e)
                    }
        
        except Exception as e:
            logger.error(f"‚ùå Erro geral nos testes de API: {e}")
            api_results["general_error"] = str(e)
        
        self.results["api_tests"] = api_results
    
    async def _run_performance_tests(self):
        """Testes de performance do sistema."""
        
        performance_results = {}
        
        try:
            async with get_async_session() as db:
                cluster_service = ClusterService(db)
                
                # Test 1: Performance de an√°lise individual
                logger.info("‚ö° Teste 1: Performance - An√°lise individual")
                try:
                    sample_clusters = TEST_CONFIG["sample_cluster_ids"]
                    if sample_clusters and cluster_service.quality_metrics_service:
                        test_cluster_id = sample_clusters[0]
                        
                        start_time = datetime.now()
                        quality_analysis = await cluster_service.analyze_cluster_quality(
                            test_cluster_id, include_detailed_analysis=True
                        )
                        end_time = datetime.now()
                        
                        duration = (end_time - start_time).total_seconds()
                        
                        performance_results["individual_analysis"] = {
                            "status": "passed" if quality_analysis else "failed",
                            "cluster_tested": test_cluster_id,
                            "duration_seconds": round(duration, 3),
                            "performance_rating": "excellent" if duration < 2 else "good" if duration < 5 else "slow"
                        }
                        
                        logger.info(f"‚úÖ An√°lise individual: {duration:.3f}s")
                    else:
                        performance_results["individual_analysis"] = {
                            "status": "skipped",
                            "reason": "Clusters ou servi√ßo n√£o dispon√≠veis"
                        }
                        
                except Exception as e:
                    performance_results["individual_analysis"] = {
                        "status": "failed",
                        "error": str(e)
                    }
                
                # Test 2: Performance de an√°lise em lote
                logger.info("‚ö° Teste 2: Performance - An√°lise em lote")
                try:
                    start_time = datetime.now()
                    batch_analysis = await cluster_service.analyze_all_clusters_quality(
                        cluster_type="case", batch_size=3  # Pequeno para teste
                    )
                    end_time = datetime.now()
                    
                    duration = (end_time - start_time).total_seconds()
                    clusters_analyzed = batch_analysis.get("total_analyzed", 0) if batch_analysis else 0
                    
                    performance_results["batch_analysis"] = {
                        "status": "passed" if batch_analysis else "failed",
                        "duration_seconds": round(duration, 3),
                        "clusters_analyzed": clusters_analyzed,
                        "avg_time_per_cluster": round(duration / max(1, clusters_analyzed), 3),
                        "performance_rating": "excellent" if duration < 10 else "good" if duration < 30 else "slow"
                    }
                    
                    logger.info(f"‚úÖ An√°lise em lote: {duration:.3f}s para {clusters_analyzed} clusters")
                    
                except Exception as e:
                    performance_results["batch_analysis"] = {
                        "status": "failed",
                        "error": str(e)
                    }
        
        except Exception as e:
            logger.error(f"‚ùå Erro geral nos testes de performance: {e}")
            performance_results["general_error"] = str(e)
        
        self.results["performance_tests"] = performance_results
    
    def _generate_final_report(self):
        """Gera relat√≥rio final dos testes."""
        
        total_duration = (datetime.now() - self.test_start_time).total_seconds()
        
        # Calcular estat√≠sticas
        all_test_categories = ["unit_tests", "integration_tests", "api_tests", "performance_tests"]
        passed_tests = 0
        failed_tests = 0
        skipped_tests = 0
        total_tests = 0
        
        for category in all_test_categories:
            tests = self.results.get(category, {})
            for test_name, test_result in tests.items():
                if isinstance(test_result, dict) and "status" in test_result:
                    total_tests += 1
                    status = test_result["status"]
                    if status == "passed":
                        passed_tests += 1
                    elif status == "failed":
                        failed_tests += 1
                    elif status == "skipped":
                        skipped_tests += 1
        
        # Determinar status geral
        if failed_tests == 0 and passed_tests > 0:
            overall_status = "passed"
        elif failed_tests > 0 and passed_tests > failed_tests:
            overall_status = "partial"
        else:
            overall_status = "failed"
        
        # Adicionar summary ao resultado
        self.results["test_summary"] = {
            "overall_status": overall_status,
            "total_duration_seconds": round(total_duration, 3),
            "total_tests": total_tests,
            "passed_tests": passed_tests,
            "failed_tests": failed_tests,
            "skipped_tests": skipped_tests,
            "success_rate": round((passed_tests / max(1, total_tests)) * 100, 1),
            "test_categories": all_test_categories,
            "completed_at": datetime.now().isoformat()
        }
        
        self.results["overall_status"] = overall_status
        
        # Log do resumo
        logger.info("=" * 60)
        logger.info("üìä RELAT√ìRIO FINAL DOS TESTES")
        logger.info("=" * 60)
        logger.info(f"Status Geral: {overall_status.upper()}")
        logger.info(f"Dura√ß√£o Total: {total_duration:.3f}s")
        logger.info(f"Testes Executados: {total_tests}")
        logger.info(f"‚úÖ Passou: {passed_tests}")
        logger.info(f"‚ùå Falhou: {failed_tests}")
        logger.info(f"‚è≠Ô∏è Pulado: {skipped_tests}")
        logger.info(f"Taxa de Sucesso: {(passed_tests / max(1, total_tests)) * 100:.1f}%")
        logger.info("=" * 60)


# Fun√ß√µes de teste espec√≠ficas
async def test_unit_only():
    """Executa apenas testes unit√°rios."""
    tester = ClusterQualitySystemTester()
    await tester._run_unit_tests()
    return tester.results

async def test_integration_only():
    """Executa apenas testes de integra√ß√£o."""
    tester = ClusterQualitySystemTester()
    await tester._run_integration_tests()
    return tester.results

async def test_api_only():
    """Executa apenas testes de API."""
    tester = ClusterQualitySystemTester()
    await tester._run_api_tests()
    return tester.results


# Fun√ß√£o principal
async def main():
    """Fun√ß√£o principal do script de teste."""
    
    parser = argparse.ArgumentParser(
        description="Testa o sistema de m√©tricas de qualidade dos clusters"
    )
    parser.add_argument(
        "--test-type",
        choices=["all", "unit", "integration", "api", "performance"],
        default="all",
        help="Tipo de teste a executar"
    )
    parser.add_argument(
        "--output-file",
        type=str,
        help="Arquivo para salvar resultados em JSON"
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Output verbose"
    )
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    logger.info(f"üéØ Executando testes: {args.test_type}")
    
    # Executar testes baseado no tipo
    tester = ClusterQualitySystemTester()
    
    if args.test_type == "all":
        results = await tester.run_all_tests()
    elif args.test_type == "unit":
        results = await test_unit_only()
    elif args.test_type == "integration":
        results = await test_integration_only() 
    elif args.test_type == "api":
        results = await test_api_only()
    elif args.test_type == "performance":
        await tester._run_performance_tests()
        results = tester.results
    else:
        logger.error(f"‚ùå Tipo de teste desconhecido: {args.test_type}")
        return
    
    # Salvar resultados se solicitado
    if args.output_file:
        try:
            with open(args.output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            logger.info(f"üìÅ Resultados salvos em: {args.output_file}")
        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar resultados: {e}")
    
    # C√≥digo de sa√≠da baseado no resultado
    overall_status = results.get("overall_status", "failed")
    if overall_status == "passed":
        sys.exit(0)
    elif overall_status == "partial":
        sys.exit(1)
    else:
        sys.exit(2)


if __name__ == "__main__":
    if not IMPORTS_AVAILABLE:
        logger.error("‚ùå N√£o foi poss√≠vel importar os m√≥dulos necess√°rios")
        sys.exit(3)
    
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("üõë Testes interrompidos pelo usu√°rio")
        sys.exit(130)
    except Exception as e:
        logger.error(f"‚ùå Erro fatal: {e}")
        sys.exit(1) 