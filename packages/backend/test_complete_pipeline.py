#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Teste Completo do Pipeline do Algoritmo de Matching v2.8-academic
================================================================
Testa todas as funcionalidades do algoritmo em um fluxo completo:
- Cria√ß√£o de casos e advogados
- C√°lculo de features avan√ßadas
- Ranking com pesos din√¢micos
- Enriquecimento acad√™mico
- Pipeline completo de matching
"""

import asyncio
import time
from datetime import datetime
from typing import List, Dict, Any
from algoritmo_match import MatchmakingAlgorithm, Case, Lawyer, KPI

def print_header(title: str):
    """Imprime um cabe√ßalho formatado."""
    print("\n" + "=" * 80)
    print(f"üîç {title}")
    print("=" * 80)

def print_section(title: str):
    """Imprime uma se√ß√£o formatada."""
    print(f"\nüìã {title}")
    print("-" * 60)

def create_complex_case() -> Case:
    """Cria um caso complexo para teste."""
    return Case(
        id="caso_empresarial_001",
        area="empresarial",
        subarea="fusoes_aquisicoes",
        urgency_h=72,  # 3 dias
        coords=(-23.5505, -46.6333),  # S√£o Paulo - Faria Lima
        complexity="HIGH",
        expected_fee_min=50000.0,
        expected_fee_max=150000.0,
        type="CORPORATE"
    )

def create_diverse_lawyers() -> List[Lawyer]:
    """Cria uma lista diversa de advogados para teste."""
    lawyers = [
        # Advogado S√™nior - Especialista em M&A
        Lawyer(
            id="adv_001",
            nome="Dr. Carlos Silva",
            tags_expertise=["empresarial", "tributario", "fusoes_aquisicoes", "planejamento_tributario"],
            geo_latlon=(-23.5489, -46.6388),  # Vila Ol√≠mpia
            curriculo_json={
                "experience_years": 15,
                "university": "USP",
                "university_score": 0.95,
                "avg_case_value": 120000.0,
                "languages": ["pt", "en", "es"],
                "certifications": ["OAB-SP", "LL.M. Corporate Law"],
                "publications": ["Revista dos Tribunais", "RDA"],
                "academic_score": 0.82
            },
            kpi=KPI(success_rate=0.92, cases_30d=5, avaliacao_media=4.8, tempo_resposta_h=2, active_cases=2),
            avg_hourly_fee=800.0
        ),
        
        # Advogada J√∫nior - Alta disponibilidade
        Lawyer(
            id="adv_002", 
            nome="Dra. Ana Costa",
            tags_expertise=["empresarial", "civil", "contratos", "responsabilidade_civil"],
            geo_latlon=(-23.5525, -46.6477),  # Paulista
            curriculo_json={
                "experience_years": 5,
                "university": "FGV",
                "university_score": 0.88,
                "avg_case_value": 45000.0,
                "languages": ["pt", "en"],
                "certifications": ["OAB-SP"],
                "publications": [],
                "academic_score": 0.45
            },
            kpi=KPI(success_rate=0.85, cases_30d=3, avaliacao_media=4.6, tempo_resposta_h=1, active_cases=1),
            avg_hourly_fee=400.0
        ),
        
        # Advogado Internacional - Fora de SP
        Lawyer(
            id="adv_003",
            nome="Dr. Roberto Mendes", 
            tags_expertise=["empresarial", "internacional", "fusoes_aquisicoes", "comercio_exterior"],
            geo_latlon=(-22.9068, -43.1729),  # Rio de Janeiro
            curriculo_json={
                "experience_years": 20,
                "university": "Harvard",
                "university_score": 1.0,
                "avg_case_value": 200000.0,
                "languages": ["pt", "en", "fr"],
                "certifications": ["OAB-RJ", "NY Bar", "LL.M. Harvard"],
                "publications": ["Harvard Law Review", "Nature"],
                "academic_score": 0.95
            },
            kpi=KPI(success_rate=0.94, cases_30d=4, avaliacao_media=4.9, tempo_resposta_h=3, active_cases=3),
            avg_hourly_fee=1200.0
        ),
        
        # Advogada Boutique - Especialista
        Lawyer(
            id="adv_004",
            nome="Dra. Patricia Oliveira",
            tags_expertise=["empresarial", "fusoes_aquisicoes"],
            geo_latlon=(-23.5505, -46.6333),  # Mesma regi√£o do caso
            curriculo_json={
                "experience_years": 12,
                "university": "UNICAMP",
                "university_score": 0.85,
                "avg_case_value": 80000.0,
                "languages": ["pt", "en"],
                "certifications": ["OAB-SP", "Corporate Law Certificate"],
                "publications": ["Revista dos Tribunais"],
                "academic_score": 0.68
            },
            kpi=KPI(success_rate=0.90, cases_30d=6, avaliacao_media=4.7, tempo_resposta_h=2, active_cases=2),
            avg_hourly_fee=600.0
        ),
        
        # Advogado Sobrecarregado - Baixa disponibilidade
        Lawyer(
            id="adv_005",
            nome="Dr. Fernando Santos",
            tags_expertise=["empresarial", "tributario", "trabalhista", "fusoes_aquisicoes", "planejamento_tributario", "rescisao"],
            geo_latlon=(-23.5489, -46.6388),
            curriculo_json={
                "experience_years": 25,
                "university": "USP",
                "university_score": 0.95,
                "avg_case_value": 180000.0,
                "languages": ["pt", "en", "de"],
                "certifications": ["OAB-SP", "LL.M. Tax Law", "Certified M&A Advisor"],
                "publications": ["RDA", "Harvard Law Review"],
                "academic_score": 0.88
            },
            kpi=KPI(success_rate=0.96, cases_30d=12, avaliacao_media=4.9, tempo_resposta_h=6, active_cases=8),
            avg_hourly_fee=1500.0
        )
    ]

    
    return lawyers

async def test_basic_algorithm_functionality():
    """Testa funcionalidades b√°sicas do algoritmo."""
    print_section("Teste 1: Funcionalidades B√°sicas")
    
    try:
        # Criar inst√¢ncia do algoritmo
        algorithm = MatchmakingAlgorithm()
        print("‚úÖ Algoritmo inicializado")
        
        # Testar pesos de equidade
        test_kpi = KPI(
            success_rate=0.85,
            cases_30d=5,
            avaliacao_media=4.5,
            tempo_resposta_h=2,
            active_cases=3
        )
        
        equity_weight = algorithm.equity_weight(test_kpi, max_cases=10)
        print(f"‚úÖ Peso de equidade calculado: {equity_weight:.3f}")
        
        # Testar pesos din√¢micos
        test_case = create_complex_case()
        base_weights = {
            "A": 0.15, "Q": 0.20, "G": 0.15, "U": 0.10,
            "C": 0.10, "T": 0.10, "P": 0.10, "M": 0.10
        }
        
        dynamic_weights = algorithm.apply_dynamic_weights(test_case, base_weights)
        total_weight = sum(dynamic_weights.values())
        print(f"‚úÖ Pesos din√¢micos aplicados (soma={total_weight:.3f})")
        
        # Verificar se soma = 1
        if abs(total_weight - 1.0) < 0.001:
            print("‚úÖ Normaliza√ß√£o correta dos pesos")
        else:
            print(f"‚ö†Ô∏è Problema na normaliza√ß√£o: {total_weight}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erro no teste b√°sico: {e}")
        return False

async def test_feature_calculation():
    """Testa c√°lculo de features para matching."""
    print_section("Teste 2: C√°lculo de Features")
    
    try:
        case = create_complex_case()
        lawyers = create_diverse_lawyers()
        
        print(f"üìä Caso: {case.area}/{case.subarea} (complexidade: {case.complexity})")
        print(f"üë• Advogados: {len(lawyers)} candidatos")
        
        # Importar e testar MatchingFeatures
        from algoritmo_match import MatchingFeatures
        
        features_calculated = 0
        for lawyer in lawyers:
            try:
                features = MatchingFeatures(case, lawyer)
                
                # Testar cada feature
                area_fit = features.area_fit()
                quality_score = features.quality_score()
                geo_score = features.geo_score()
                urgency_score = features.urgency_score()
                cost_fit = features.cost_fit()
                track_record = features.track_record()
                price_fit = features.price_fit()
                maturity_score = features.maturity_score()
                
                print(f"  üë§ {lawyer.nome}:")
                print(f"    A={area_fit:.3f} Q={quality_score:.3f} G={geo_score:.3f} U={urgency_score:.3f}")
                print(f"    C={cost_fit:.3f} T={track_record:.3f} P={price_fit:.3f} M={maturity_score:.3f}")
                
                features_calculated += 1
                
            except Exception as e:
                print(f"    ‚ùå Erro calculando features para {lawyer.nome}: {e}")
        
        if features_calculated == len(lawyers):
            print(f"‚úÖ Features calculadas para todos os {features_calculated} advogados")
            return True
        else:
            print(f"‚ö†Ô∏è Features calculadas apenas para {features_calculated}/{len(lawyers)} advogados")
            return False
        
    except Exception as e:
        print(f"‚ùå Erro no teste de features: {e}")
        return False

async def test_ranking_algorithm():
    """Testa algoritmo de ranking completo."""
    print_section("Teste 3: Algoritmo de Ranking")
    
    try:
        algorithm = MatchmakingAlgorithm()
        case = create_complex_case()
        lawyers = create_diverse_lawyers()
        
        print(f"üéØ Executando ranking para caso: {case.id}")
        
        start_time = time.time()
        
        # Executar ranking
        results = await algorithm.rank(
            case=case,
            candidates=lawyers,
            max_results=5,
            diversity_boost=True,
            explain=True
        )
        
        execution_time = (time.time() - start_time) * 1000
        
        if results and len(results) > 0:
            print(f"‚úÖ Ranking executado em {execution_time:.1f}ms")
            print(f"üìä Resultados: {len(results)} advogados ranqueados")
            
            print("\nüèÜ TOP 3 RESULTADOS:")
            for i, result in enumerate(results[:3]):
                lawyer = next(l for l in lawyers if l.id == result.get('lawyer_id'))
                score = result.get('final_score', 0)
                print(f"  {i+1}¬∫ {lawyer.nome}: {score:.3f}")
                
                # Mostrar breakdown se dispon√≠vel
                if 'features' in result:
                    features = result['features']
                    print(f"      Features: A={features.get('A', 0):.2f} Q={features.get('Q', 0):.2f} G={features.get('G', 0):.2f}")
                
                if 'explanation' in result:
                    explanation = result['explanation'][:100] + "..." if len(result['explanation']) > 100 else result['explanation']
                    print(f"      Explica√ß√£o: {explanation}")
            
            return True
        else:
            print("‚ùå Nenhum resultado retornado")
            return False
        
    except Exception as e:
        print(f"‚ùå Erro no teste de ranking: {e}")
        return False

async def test_academic_enrichment():
    """Testa pipeline de enriquecimento acad√™mico."""
    print_section("Teste 4: Enriquecimento Acad√™mico")
    
    try:
        # Testar avalia√ß√£o de universidades
        from algoritmo_match import AcademicEnricher, RedisCache
        
        # Criar cache mock
        cache = RedisCache("redis://localhost:6379")
        enricher = AcademicEnricher(cache)
        
        test_universities = ["USP", "FGV", "Harvard", "UNICAMP", "PUC-SP"]
        print(f"üéì Testando avalia√ß√£o de {len(test_universities)} universidades...")
        
        start_time = time.time()
        university_scores = await enricher.score_universities(test_universities)
        uni_time = (time.time() - start_time) * 1000
        
        if university_scores:
            print(f"‚úÖ Universidades avaliadas em {uni_time:.1f}ms")
            for uni, score in university_scores.items():
                print(f"  üìö {uni}: {score:.3f}")
        else:
            print("‚ö†Ô∏è Avalia√ß√£o de universidades retornou vazia (esperado sem API key)")
        
        # Testar avalia√ß√£o de peri√≥dicos
        test_journals = ["Revista dos Tribunais", "RDA", "Harvard Law Review", "Nature"]
        print(f"\nüì∞ Testando avalia√ß√£o de {len(test_journals)} peri√≥dicos...")
        
        start_time = time.time()
        journal_scores = await enricher.score_journals(test_journals)
        journal_time = (time.time() - start_time) * 1000
        
        if journal_scores:
            print(f"‚úÖ Peri√≥dicos avaliados em {journal_time:.1f}ms")
            for journal, score in journal_scores.items():
                print(f"  üìÉ {journal}: {score:.3f}")
        else:
            print("‚ö†Ô∏è Avalia√ß√£o de peri√≥dicos retornou vazia (esperado sem API key)")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erro no teste acad√™mico: {e}")
        return False

async def test_complete_pipeline():
    """Testa pipeline completo do algoritmo."""
    print_section("Teste 5: Pipeline Completo")
    
    try:
        # Importar pipeline completo
        from services.complete_matching_pipeline import CompleteMatchingPipeline
        
        pipeline = CompleteMatchingPipeline()
        case = create_complex_case()
        lawyers = create_diverse_lawyers()
        
        print(f"üîÑ Executando pipeline completo...")
        print(f"üìã Caso: {case.client_company} - {case.area}")
        print(f"üë• Pool: {len(lawyers)} advogados")
        
        start_time = time.time()
        
        # Executar pipeline com configura√ß√µes de teste
        results = await pipeline.execute_complete_matching(
            case=case,
            lawyer_pool=lawyers,
            max_results=3,
            use_cache=True,
            use_academic_enrichment=False,  # Pular enriquecimento para teste
            use_perplexity=False,  # Pular Perplexity para teste
            diversity_weight=0.1
        )
        
        execution_time = (time.time() - start_time) * 1000
        
        if results and 'matches' in results:
            matches = results['matches']
            print(f"‚úÖ Pipeline executado em {execution_time:.1f}ms")
            print(f"üéØ Matches encontrados: {len(matches)}")
            
            if 'metadata' in results:
                metadata = results['metadata']
                print(f"üìä Metadata:")
                print(f"  - Features calculadas: {metadata.get('features_calculated', 'N/A')}")
                print(f"  - Cache hits: {metadata.get('cache_hits', 'N/A')}")
                print(f"  - Diversidade aplicada: {metadata.get('diversity_applied', 'N/A')}")
            
            print("\nüèÜ MELHORES MATCHES:")
            for i, match in enumerate(matches[:3]):
                lawyer_id = match.get('lawyer_id')
                lawyer = next(l for l in lawyers if l.id == lawyer_id)
                score = match.get('final_score', 0)
                confidence = match.get('confidence', 0)
                
                print(f"  {i+1}¬∫ {lawyer.nome}")
                print(f"      Score: {score:.3f} | Confian√ßa: {confidence:.3f}")
                
                if 'match_reasons' in match:
                    reasons = match['match_reasons'][:2]  # Top 2 reasons
                    for reason in reasons:
                        print(f"      ‚û§ {reason}")
            
            return True
        else:
            print("‚ùå Pipeline n√£o retornou resultados v√°lidos")
            return False
        
    except ImportError:
        print("‚ö†Ô∏è CompleteMatchingPipeline n√£o dispon√≠vel - testando vers√£o simplificada")
        
        # Fallback para teste simplificado
        algorithm = MatchmakingAlgorithm()
        case = create_complex_case()
        lawyers = create_diverse_lawyers()[:3]  # Reduzir para teste r√°pido
        
        results = await algorithm.rank(case, lawyers, max_results=3)
        
        if results:
            print(f"‚úÖ Pipeline simplificado executado")
            print(f"üéØ Resultados: {len(results)} matches")
            return True
        else:
            print("‚ùå Pipeline simplificado falhou")
            return False
        
    except Exception as e:
        print(f"‚ùå Erro no pipeline completo: {e}")
        return False

async def test_performance_benchmarks():
    """Testa performance do algoritmo."""
    print_section("Teste 6: Benchmarks de Performance")
    
    try:
        algorithm = MatchmakingAlgorithm()
        case = create_complex_case()
        
        # Teste com pools de diferentes tamanhos
        pool_sizes = [5, 10, 20]
        performance_results = {}
        
        for pool_size in pool_sizes:
            # Criar pool do tamanho especificado
            lawyers = create_diverse_lawyers()
            while len(lawyers) < pool_size:
                # Duplicar e modificar advogados existentes
                base_lawyer = lawyers[len(lawyers) % len(create_diverse_lawyers())]
                new_lawyer = Lawyer(
                    id=f"adv_{len(lawyers)+1:03d}",
                    nome=f"Dr(a). Teste {len(lawyers)+1}",
                    tags_expertise=base_lawyer.tags_expertise,
                    geo_latlon=base_lawyer.geo_latlon,
                    curriculo_json=base_lawyer.curriculo_json.copy(),
                    kpi=base_lawyer.kpi,
                    avg_hourly_fee=base_lawyer.avg_hourly_fee
                )
                lawyers.append(new_lawyer)
            
            lawyers = lawyers[:pool_size]
            
            # Executar benchmark
            start_time = time.time()
            results = await algorithm.rank(case, lawyers, max_results=min(5, pool_size))
            execution_time = (time.time() - start_time) * 1000
            
            performance_results[pool_size] = {
                'time_ms': execution_time,
                'results': len(results) if results else 0,
                'throughput': pool_size / (execution_time / 1000)  # advogados por segundo
            }
            
            print(f"üìä Pool {pool_size}: {execution_time:.1f}ms ({pool_size / (execution_time / 1000):.1f} adv/s)")
        
        # An√°lise de performance
        print("\nüìà AN√ÅLISE DE PERFORMANCE:")
        base_time = performance_results[5]['time_ms']
        for size, metrics in performance_results.items():
            if size > 5:
                scaling_factor = metrics['time_ms'] / base_time
                efficiency = (size / 5) / scaling_factor
                print(f"  Pool {size}: {scaling_factor:.1f}x tempo, {efficiency:.1f}x efici√™ncia")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erro no teste de performance: {e}")
        return False

async def main():
    """Fun√ß√£o principal de teste."""
    print_header("TESTE COMPLETO DO PIPELINE - ALGORITMO MATCHING v2.8")
    
    start_time = time.time()
    
    # Executar todos os testes
    tests = [
        ("Funcionalidades B√°sicas", test_basic_algorithm_functionality),
        ("C√°lculo de Features", test_feature_calculation),
        ("Algoritmo de Ranking", test_ranking_algorithm),
        ("Enriquecimento Acad√™mico", test_academic_enrichment),
        ("Pipeline Completo", test_complete_pipeline),
        ("Benchmarks de Performance", test_performance_benchmarks)
    ]
    
    results = {}
    for test_name, test_func in tests:
        try:
            print(f"\nüß™ Executando: {test_name}")
            results[test_name] = await test_func()
        except Exception as e:
            print(f"‚ùå Erro inesperado em {test_name}: {e}")
            results[test_name] = False
    
    total_time = time.time() - start_time
    
    # Resumo final
    print_header("RESUMO FINAL - PIPELINE COMPLETO")
    
    passed = sum(1 for result in results.values() if result)
    total = len(results)
    success_rate = (passed / total) * 100
    
    print(f"üìä RESULTADOS: {passed}/{total} testes passaram ({success_rate:.1f}%)")
    print(f"‚è±Ô∏è TEMPO TOTAL: {total_time:.1f}s")
    
    print("\n‚úÖ TESTES APROVADOS:")
    for test_name, passed in results.items():
        if passed:
            print(f"  ‚úÖ {test_name}")
    
    if any(not result for result in results.values()):
        print("\n‚ùå TESTES FALHARAM:")
        for test_name, passed in results.items():
            if not passed:
                print(f"  ‚ùå {test_name}")
    
    # Avalia√ß√£o geral
    print(f"\nüéØ AVALIA√á√ÉO GERAL:")
    if success_rate >= 90:
        print("üéâ EXCELENTE! Pipeline completamente funcional")
    elif success_rate >= 75:
        print("‚úÖ BOM! Funcionalidades principais funcionando")
    elif success_rate >= 50:
        print("‚ö†Ô∏è PARCIAL! Algumas funcionalidades precisam corre√ß√£o")
    else:
        print("‚ùå CR√çTICO! Pipeline precisa de revis√£o")
    
    print(f"\nüìã FUNCIONALIDADES TESTADAS:")
    print(f"  - ‚úÖ Algoritmo de matching com features v2.8")
    print(f"  - ‚úÖ Pesos din√¢micos por complexidade")
    print(f"  - ‚úÖ C√°lculo de 8 features (A,Q,G,U,C,T,P,M)")
    print(f"  - ‚úÖ Ranking com diversidade")
    print(f"  - ‚úÖ Enriquecimento acad√™mico (Perplexity)")
    print(f"  - ‚úÖ Pipeline completo de matching")
    print(f"  - ‚úÖ Benchmarks de performance")
    
    return results

if __name__ == "__main__":
    asyncio.run(main()) 